{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg02FZzDyEqd"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:34.289130Z",
     "iopub.status.busy": "2020-09-17T01:33:34.288557Z",
     "iopub.status.idle": "2020-09-17T01:33:34.290255Z",
     "shell.execute_reply": "2020-09-17T01:33:34.290746Z"
    },
    "id": "2mapZ9afGJ69"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- fix model creation (preprocessing breaks tensor? https://tensorflow.google.cn/tutorials/load_data/csv)\n",
    "\n",
    "- name all layers\n",
    "\n",
    "- save model for loading afterwards (currently broken, might require named layers, might clash with concatenate layer)\n",
    "\n",
    "- make sure to mark unaltered cells and annotations as from the original tutorial\n",
    "\n",
    "- make sure the GPU is used\n",
    "\n",
    "- make model .fit and .evaluate work (if it does not)\n",
    "\n",
    "- make callbacks work during fitting (in custom loop)\n",
    "\n",
    "\n",
    "Maybe:\n",
    "\n",
    "- make a preprocessing model where the preprocessing is done when SENN is initialized\n",
    "(for portability)\n",
    "\n",
    "- augment data by just adding random noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMYQvJuBi7MS"
   },
   "source": [
    "# Classify structured data using Keras Preprocessing Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FaL4wnr22oy"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/preprocessing_layers.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/preprocessing_layers.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/structured_data/preprocessing_layers.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nna1tOKxyEqe"
   },
   "source": [
    "This tutorial demonstrates how to classify structured data (e.g. tabular data in a CSV). You will use [Keras](https://www.tensorflow.org/guide/keras) to define the model, and [preprocessing layers](https://keras.io/guides/preprocessing_layers/) as a bridge to map from columns in a CSV to features used to train the model. This tutorial contains complete code to:\n",
    "\n",
    "* Load a CSV file using [Pandas](https://pandas.pydata.org/).\n",
    "* Build an input pipeline to batch and shuffle the rows using [tf.data](https://www.tensorflow.org/guide/datasets).\n",
    "* Map from columns in the CSV to features used to train the model using Keras Preprocessing layers.\n",
    "* Build, train, and evaluate a model using Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5xkXCicjFQD"
   },
   "source": [
    "Note: This tutorial is similar to [Classify structured data with feature columns](https://www.tensorflow.org/tutorials/structured_data/feature_columns). This version uses new experimental Keras [Preprocessing Layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing) instead of `tf.feature_column`. Keras Preprocessing Layers are more intuitive, and can be easily included inside your model to simplify deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHxU1FMNpomc"
   },
   "source": [
    "## The Dataset\n",
    "\n",
    "For designing the network I use a smaller and simpler dataset. \n",
    "\n",
    "It is a simplified version of the PetFinder [dataset](https://www.kaggle.com/c/petfinder-adoption-prediction). There are several thousand rows in the CSV. Each row describes a pet, and each column describes an attribute. \n",
    "\n",
    "The goal is to predict if the pet will be adopted.\n",
    "\n",
    "Following is a description of this dataset. \\\n",
    "Notice there are both numeric and categorical columns. \n",
    "\n",
    "The free text column will be ignored.\n",
    "\n",
    "Column | Description| Feature Type | Data Type\n",
    "------------|--------------------|----------------------|-----------------\n",
    "Type | Type of animal (Dog, Cat) | Categorical | string\n",
    "Age |  Age of the pet | Numerical | integer\n",
    "Breed1 | Primary breed of the pet | Categorical | string\n",
    "Color1 | Color 1 of pet | Categorical | string\n",
    "Color2 | Color 2 of pet | Categorical | string\n",
    "MaturitySize | Size at maturity | Categorical | string\n",
    "FurLength | Fur length | Categorical | string\n",
    "Vaccinated | Pet has been vaccinated | Categorical | string\n",
    "Sterilized | Pet has been sterilized | Categorical | string\n",
    "Health | Health Condition | Categorical | string\n",
    "Fee | Adoption Fee | Numerical | integer\n",
    "Description | Profile write-up for this pet | Text | string\n",
    "PhotoAmt | Total uploaded photos for this pet | Numerical | integer\n",
    "AdoptionSpeed | Speed of adoption | Classification | integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjFbdBldyEqf"
   },
   "source": [
    "## Install and Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:34.295537Z",
     "iopub.status.busy": "2020-09-17T01:33:34.294862Z",
     "iopub.status.idle": "2020-09-17T01:33:35.675952Z",
     "shell.execute_reply": "2020-09-17T01:33:35.675219Z"
    },
    "id": "S_BdyQlPjfDW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from scikit-learn->sklearn) (1.5.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (1.18.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (1.1.4)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from pandas) (2020.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (1.33.2)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from protobuf>=3.9.2->tensorflow) (50.3.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.11.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: pydot in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from pydot) (2.4.7)\n",
      "Requirement already satisfied: pydotplus in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from pydotplus) (2.4.7)\n",
      "Requirement already satisfied: graphviz in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (0.14.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datetime in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (4.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from datetime) (2020.4)\n",
      "Requirement already satisfied: zope.interface in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from datetime) (5.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from zope.interface->datetime) (50.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (20.4)\n",
      "Requirement already satisfied: six in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from packaging) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from packaging) (2.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from keras) (1.5.4)\n",
      "Requirement already satisfied: h5py in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from h5py->keras) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install tensorflow\n",
    "!pip install pydot\n",
    "!pip install pydotplus\n",
    "!pip install graphviz\n",
    "!pip install datetime\n",
    "!pip install packaging\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install for graph: https://graphviz.gitlab.io/download/\n",
    "maybe follow: https://bobswift.atlassian.net/wiki/spaces/GVIZ/pages/131924165/Graphviz+installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from datetime import datetime\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXvBvobayEqi"
   },
   "source": [
    "## Reading in the data\n",
    "\n",
    "The data is read into a pandas dataframe\n",
    "\n",
    "Again:\\\n",
    "As the real data is sensitive, large and expensive to use,\n",
    "for now I use a dummy dataset about adoption-speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:42.197790Z",
     "iopub.status.busy": "2020-09-17T01:33:42.197028Z",
     "iopub.status.idle": "2020-09-17T01:33:42.321623Z",
     "shell.execute_reply": "2020-09-17T01:33:42.320979Z"
    },
    "id": "qJ4Ajn-YyEqj",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\n",
    "csv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n",
    "\n",
    "tf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n",
    "                        extract=True, cache_dir='.')\n",
    "dataframe = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:42.343796Z",
     "iopub.status.busy": "2020-09-17T01:33:42.343010Z",
     "iopub.status.idle": "2020-09-17T01:33:42.352803Z",
     "shell.execute_reply": "2020-09-17T01:33:42.353374Z"
    },
    "id": "3uiq4hoIGyXI",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Fee</th>\n",
       "      <th>Description</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cat</td>\n",
       "      <td>3</td>\n",
       "      <td>Tabby</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>White</td>\n",
       "      <td>Small</td>\n",
       "      <td>Short</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>100</td>\n",
       "      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cat</td>\n",
       "      <td>1</td>\n",
       "      <td>Domestic Medium Hair</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Not Sure</td>\n",
       "      <td>Not Sure</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>I just found it alone yesterday near my apartm...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dog</td>\n",
       "      <td>1</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Male</td>\n",
       "      <td>Brown</td>\n",
       "      <td>White</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>Their pregnant mother was dumped by her irresp...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dog</td>\n",
       "      <td>4</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Short</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>150</td>\n",
       "      <td>Good guard dog, very alert, active, obedience ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dog</td>\n",
       "      <td>1</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>No Color</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Short</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>This handsome yet cute boy is up for adoption....</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  Age                Breed1  Gender Color1    Color2 MaturitySize  \\\n",
       "0  Cat    3                 Tabby    Male  Black     White        Small   \n",
       "1  Cat    1  Domestic Medium Hair    Male  Black     Brown       Medium   \n",
       "2  Dog    1           Mixed Breed    Male  Brown     White       Medium   \n",
       "3  Dog    4           Mixed Breed  Female  Black     Brown       Medium   \n",
       "4  Dog    1           Mixed Breed    Male  Black  No Color       Medium   \n",
       "\n",
       "  FurLength Vaccinated Sterilized   Health  Fee  \\\n",
       "0     Short         No         No  Healthy  100   \n",
       "1    Medium   Not Sure   Not Sure  Healthy    0   \n",
       "2    Medium        Yes         No  Healthy    0   \n",
       "3     Short        Yes         No  Healthy  150   \n",
       "4     Short         No         No  Healthy    0   \n",
       "\n",
       "                                         Description  PhotoAmt  AdoptionSpeed  \n",
       "0  Nibble is a 3+ month old ball of cuteness. He ...         1              2  \n",
       "1  I just found it alone yesterday near my apartm...         2              0  \n",
       "2  Their pregnant mother was dumped by her irresp...         7              3  \n",
       "3  Good guard dog, very alert, active, obedience ...         8              2  \n",
       "4  This handsome yet cute boy is up for adoption....         3              2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3zDbrozyEqq"
   },
   "source": [
    "## Creating the target variable\n",
    "\n",
    "I have to select the variable I want to train for and drop the columns that are not important or contain that information from the normal dataset.\n",
    "\n",
    "Valid for the example data:\n",
    "The task in the Kaggle competition was to predict the speed at which a pet will be adopted (e.g., in the first week, the first month, the first three months, and so on). Let's simplify this for our purposes. It is transformed into a binary classification problem:\n",
    "I simply predict whether the pet was adopted, or not.\n",
    "\n",
    "After modifying the label column, 0 will indicate the pet was not adopted, and 1 will indicate it was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:42.358975Z",
     "iopub.status.busy": "2020-09-17T01:33:42.358305Z",
     "iopub.status.idle": "2020-09-17T01:33:42.365151Z",
     "shell.execute_reply": "2020-09-17T01:33:42.364619Z"
    },
    "id": "wmMDc46-yEqq",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# In the original dataset \"4\" indicates the pet was not adopted.\n",
    "dataframe['target'] = np.where(dataframe['AdoptionSpeed']==4, 0, 1)\n",
    "\n",
    "# Drop un-used columns. (including our now target which can not be used for training)\n",
    "dataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataframe = dataframe.drop(columns=['Fee', 'PhotoAmt','Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\n",
    "#                    'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Breed1'])\n",
    "\n",
    "#for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Fee</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cat</td>\n",
       "      <td>3</td>\n",
       "      <td>Tabby</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>White</td>\n",
       "      <td>Small</td>\n",
       "      <td>Short</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cat</td>\n",
       "      <td>1</td>\n",
       "      <td>Domestic Medium Hair</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Not Sure</td>\n",
       "      <td>Not Sure</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dog</td>\n",
       "      <td>1</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Male</td>\n",
       "      <td>Brown</td>\n",
       "      <td>White</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dog</td>\n",
       "      <td>4</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Short</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dog</td>\n",
       "      <td>1</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>No Color</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Short</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  Age                Breed1  Gender Color1    Color2 MaturitySize  \\\n",
       "0  Cat    3                 Tabby    Male  Black     White        Small   \n",
       "1  Cat    1  Domestic Medium Hair    Male  Black     Brown       Medium   \n",
       "2  Dog    1           Mixed Breed    Male  Brown     White       Medium   \n",
       "3  Dog    4           Mixed Breed  Female  Black     Brown       Medium   \n",
       "4  Dog    1           Mixed Breed    Male  Black  No Color       Medium   \n",
       "\n",
       "  FurLength Vaccinated Sterilized   Health  Fee  PhotoAmt  target  \n",
       "0     Short         No         No  Healthy  100         1       1  \n",
       "1    Medium   Not Sure   Not Sure  Healthy    0         2       1  \n",
       "2    Medium        Yes         No  Healthy    0         7       1  \n",
       "3     Short        Yes         No  Healthy  150         8       1  \n",
       "4     Short         No         No  Healthy    0         3       1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sp0NCbswyEqs"
   },
   "source": [
    "## Spliting the dataframe into train, validation, and test\n",
    "\n",
    "The loaded dataset was a single file. It has to be split into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:42.370622Z",
     "iopub.status.busy": "2020-09-17T01:33:42.369987Z",
     "iopub.status.idle": "2020-09-17T01:33:42.377950Z",
     "shell.execute_reply": "2020-09-17T01:33:42.378325Z"
    },
    "id": "qT6HdyEwyEqt",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7383 train examples\n",
      "1846 validation examples\n",
      "2308 test examples\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_7uVu-xyEqv"
   },
   "source": [
    "## Input pipeline\n",
    "\n",
    "The dataframe is wrapped with [tf.data](https://www.tensorflow.org/guide/datasets).\n",
    "This is done to easily shuffle and batch the data. \n",
    "\n",
    "If the RAM is not sufficient, tf.data could be used directly to read it from disk in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:42.383853Z",
     "iopub.status.busy": "2020-09-17T01:33:42.383254Z",
     "iopub.status.idle": "2020-09-17T01:33:42.385369Z",
     "shell.execute_reply": "2020-09-17T01:33:42.384906Z"
    },
    "id": "7r4j-1lRyEqw",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=1):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('target')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYxIXH579uS9"
   },
   "source": [
    "The general pipeline for input is finished here.\n",
    "What does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:43.803710Z",
     "iopub.status.busy": "2020-09-17T01:33:42.388965Z",
     "iopub.status.idle": "2020-09-17T01:33:43.830128Z",
     "shell.execute_reply": "2020-09-17T01:33:43.830540Z"
    },
    "id": "tYiNH-QI96Jo",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:43.836836Z",
     "iopub.status.busy": "2020-09-17T01:33:43.836165Z",
     "iopub.status.idle": "2020-09-17T01:33:43.891242Z",
     "shell.execute_reply": "2020-09-17T01:33:43.890630Z"
    },
    "id": "nFYir6S8HgIJ",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['Type', 'Age', 'Breed1', 'Gender', 'Color1', 'Color2', 'MaturitySize', 'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Fee', 'PhotoAmt']\n",
      "A batch of ages: tf.Tensor([ 4 12  1 30  2], shape=(5,), dtype=int64)\n",
      "A batch of targets: tf.Tensor([1 1 1 1 1], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "[(train_features, label_batch)] = train_ds.take(1)\n",
    "print('Every feature:', list(train_features.keys()))\n",
    "print('A batch of ages:', train_features['Age'])\n",
    "print('A batch of targets:', label_batch )\n",
    "\n",
    "# TODO currently this is targeted towards the dummy -set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geqHWW54Hmte"
   },
   "source": [
    "The dataset returns a dictionary of column names (from the dataframe) that map to column values from rows in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v50jBIuj4gb"
   },
   "source": [
    "## Preprocessing layers\n",
    "\n",
    "I will have to adapt the pipelines when I replace the dummy-code, but afterwards I will be able to input plain string data etc from new data as well.\n",
    "\n",
    "Information about the pre-processing layers for easy access when I am there:\n",
    "\n",
    "*   [`Normalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization) - Feature-wise normalization of the data.\n",
    "*   [`CategoryEncoding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/CategoryEncoding) - Category encoding layer.\n",
    "*   [`StringLookup`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/StringLookup) - Maps strings from a vocabulary to integer indices.\n",
    "*   [`IntegerLookup`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/IntegerLookup) - Maps integers from a vocabulary to integer indices.\n",
    "\n",
    "A list of available preprocessing layers can be found [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twXBSxnT66o8"
   },
   "source": [
    "### Numeric columns\n",
    "A Normalization() layer ensures that each numeric feature has a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OosUh4kTsK_q"
   },
   "source": [
    "The `get_normalization_layer` function returns a keras layer.\n",
    "It applies featurewise normalization to numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:43.896825Z",
     "iopub.status.busy": "2020-09-17T01:33:43.896137Z",
     "iopub.status.idle": "2020-09-17T01:33:43.898143Z",
     "shell.execute_reply": "2020-09-17T01:33:43.898564Z"
    },
    "id": "D6OuEKMMyEq1",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for our feature.\n",
    "  normalizer = preprocessing.Normalization()\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:43.903065Z",
     "iopub.status.busy": "2020-09-17T01:33:43.902172Z",
     "iopub.status.idle": "2020-09-17T01:33:44.842415Z",
     "shell.execute_reply": "2020-09-17T01:33:44.842862Z"
    },
    "id": "MpKgUDyk69bM",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[-0.8383716 ],\n",
       "       [-0.19748345],\n",
       "       [-0.19748345],\n",
       "       [-0.19748345],\n",
       "       [ 3.9682894 ]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo_count_col = train_features['PhotoAmt']\n",
    "layer = get_normalization_layer('PhotoAmt', train_ds)\n",
    "layer(photo_count_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foWY00YBUx9N"
   },
   "source": [
    "TODO: If I will indeed have many numeric features (hundreds, or more), it would be more efficient to concatenate them first and use a single [normalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization) layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVD--2WZ7vmh"
   },
   "source": [
    "### Categorical columns\n",
    "\n",
    "In the dummy dataset, Type is represented as a string (e.g. 'Dog', or 'Cat'). Sadly, one can not feed strings directly to a model. The preprocessing layer takes care of representing strings as a one-hot vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWlkOPwMsxdv"
   },
   "source": [
    "The `get_category_encoding_layer` function returns a layer, mapping values from a vocabulary to integer indices and one-hot encodes the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:44.850381Z",
     "iopub.status.busy": "2020-09-17T01:33:44.847119Z",
     "iopub.status.idle": "2020-09-17T01:33:44.851920Z",
     "shell.execute_reply": "2020-09-17T01:33:44.852447Z"
    },
    "id": "GmgaeRjlDoUO",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  # Create a StringLookup layer which will turn strings into integer indices\n",
    "  if dtype == 'string':\n",
    "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "  else:\n",
    "    index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Create a Discretization for our integer indices.\n",
    "  encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = feature_ds.map(index)\n",
    "\n",
    "  # Learn the space of possible indices.\n",
    "  encoder.adapt(feature_ds)\n",
    "\n",
    "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "  # layer so we can use them, or include them in the functional model later.\n",
    "  return lambda feature: encoder(index(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:44.862532Z",
     "iopub.status.busy": "2020-09-17T01:33:44.861894Z",
     "iopub.status.idle": "2020-09-17T01:33:45.538315Z",
     "shell.execute_reply": "2020-09-17T01:33:45.537801Z"
    },
    "id": "X2t2ff9K8PcT",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_col = train_features['Type']\n",
    "layer = get_category_encoding_layer('Type', train_ds, 'string')\n",
    "layer(type_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6eDongw8knz"
   },
   "source": [
    "Often, you don't want to feed a number directly into the model, but instead use a one-hot encoding of those inputs. Consider raw data that represents a pet's age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:45.546513Z",
     "iopub.status.busy": "2020-09-17T01:33:45.545857Z",
     "iopub.status.idle": "2020-09-17T01:33:46.118888Z",
     "shell.execute_reply": "2020-09-17T01:33:46.118367Z"
    },
    "id": "7FjBioQ38oNE",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_col = train_features['Age']\n",
    "category_encoding_layer = get_category_encoding_layer('Age', train_ds,\n",
    "                                                      'int64', 5)\n",
    "category_encoding_layer(type_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiE0glOPkMyh"
   },
   "source": [
    "## Choosing and preparing columns to use\n",
    "\n",
    "While we can deal with all types of data, we have to make a list of all columns for each type.\\\n",
    "That way I am able to define which layer needs to be treated how\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:46.142976Z",
     "iopub.status.busy": "2020-09-17T01:33:46.123925Z",
     "iopub.status.idle": "2020-09-17T01:33:46.180021Z",
     "shell.execute_reply": "2020-09-17T01:33:46.180497Z"
    },
    "id": "Rcv2kQTTo23h",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:46.190642Z",
     "iopub.status.busy": "2020-09-17T01:33:46.186870Z",
     "iopub.status.idle": "2020-09-17T01:33:46.368301Z",
     "shell.execute_reply": "2020-09-17T01:33:46.368852Z"
    },
    "id": "Q3RBa51VkaAn",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "# Numeric features.\n",
    "for header in ['PhotoAmt', 'Fee']:  # TODO use all headers in UMC set minus the ones I know are something else\n",
    "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "  normalization_layer = get_normalization_layer(header, train_ds)\n",
    "  encoded_numeric_col = normalization_layer(numeric_col)\n",
    "  all_inputs.append(numeric_col)\n",
    "  encoded_features.append(encoded_numeric_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:46.379746Z",
     "iopub.status.busy": "2020-09-17T01:33:46.378953Z",
     "iopub.status.idle": "2020-09-17T01:33:46.551415Z",
     "shell.execute_reply": "2020-09-17T01:33:46.550766Z"
    },
    "id": "1FOMGfZflhoA",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Categorical features encoded as integers.\n",
    "\n",
    "# TODO at the UMC data, this will be more common, some tests have a categorical scale \n",
    "# However, most of them can just be interpreted as normal numerical feature, so I won't have to overdo it\n",
    "age_col = tf.keras.Input(shape=(1,), name='Age', dtype='int64')\n",
    "encoding_layer = get_category_encoding_layer('Age', train_ds, dtype='int64',\n",
    "                                             max_tokens=5)\n",
    "encoded_age_col = encoding_layer(age_col)\n",
    "all_inputs.append(age_col)\n",
    "encoded_features.append(encoded_age_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:46.561248Z",
     "iopub.status.busy": "2020-09-17T01:33:46.560569Z",
     "iopub.status.idle": "2020-09-17T01:33:48.254306Z",
     "shell.execute_reply": "2020-09-17T01:33:48.254765Z"
    },
    "id": "K8C8xyiXm-Ie",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Categorical features encoded as string.\n",
    "categorical_cols = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\n",
    "                    'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Breed1'] \n",
    "# TODO replace this by reading the headings from the dataframe and substracting the hardcoded headings (that I know are something else))\n",
    "\n",
    "for header in categorical_cols:\n",
    "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "  encoding_layer = get_category_encoding_layer(header, train_ds, dtype='string',\n",
    "                                               max_tokens=5) # TODO maybe, this line has to be duplicated and slightly changed to accomodate for different max_tokens\n",
    "  encoded_categorical_col = encoding_layer(categorical_col)\n",
    "  all_inputs.append(categorical_col)\n",
    "  encoded_features.append(encoded_categorical_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Currently I do not think the UMC data needs to be balanced.\n",
    "# It will be evaluated on the same dataset (though a different part of it)\n",
    "# We do not have a large number of samples that are underrepresented, probably causing large inaccura\n",
    "\n",
    "#use:\n",
    "#    https://www.tensorflow.org/tutorials/structured_data/imbalanced_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHSnhz2fyEq3"
   },
   "source": [
    "## The model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Age (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Type (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Color1 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Color2 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Gender (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "MaturitySize (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FurLength (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Vaccinated (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Sterilized (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Health (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Breed1 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "PhotoAmt (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Fee (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "integer_lookup_1 (IntegerLookup (None, 1)            0           Age[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_1 (StringLookup)  (None, 1)            0           Type[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_2 (StringLookup)  (None, 1)            0           Color1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_3 (StringLookup)  (None, 1)            0           Color2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_4 (StringLookup)  (None, 1)            0           Gender[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_5 (StringLookup)  (None, 1)            0           MaturitySize[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_6 (StringLookup)  (None, 1)            0           FurLength[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_7 (StringLookup)  (None, 1)            0           Vaccinated[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_8 (StringLookup)  (None, 1)            0           Sterilized[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_9 (StringLookup)  (None, 1)            0           Health[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_10 (StringLookup) (None, 1)            0           Breed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "normalization_1 (Normalization) (None, 1)            3           PhotoAmt[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "normalization_2 (Normalization) (None, 1)            3           Fee[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_2 (CategoryEn (None, 5)            0           integer_lookup_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_3 (CategoryEn (None, 4)            0           string_lookup_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_4 (CategoryEn (None, 5)            0           string_lookup_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_5 (CategoryEn (None, 5)            0           string_lookup_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_6 (CategoryEn (None, 4)            0           string_lookup_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_7 (CategoryEn (None, 5)            0           string_lookup_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_8 (CategoryEn (None, 5)            0           string_lookup_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_9 (CategoryEn (None, 5)            0           string_lookup_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_10 (CategoryE (None, 5)            0           string_lookup_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_11 (CategoryE (None, 5)            0           string_lookup_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_12 (CategoryE (None, 5)            0           string_lookup_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 55)           0           normalization_1[0][0]            \n",
      "                                                                 normalization_2[0][0]            \n",
      "                                                                 category_encoding_2[0][0]        \n",
      "                                                                 category_encoding_3[0][0]        \n",
      "                                                                 category_encoding_4[0][0]        \n",
      "                                                                 category_encoding_5[0][0]        \n",
      "                                                                 category_encoding_6[0][0]        \n",
      "                                                                 category_encoding_7[0][0]        \n",
      "                                                                 category_encoding_8[0][0]        \n",
      "                                                                 category_encoding_9[0][0]        \n",
      "                                                                 category_encoding_10[0][0]       \n",
      "                                                                 category_encoding_11[0][0]       \n",
      "                                                                 category_encoding_12[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 6\n",
      "Trainable params: 0\n",
      "Non-trainable params: 6\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The first step towards a working model\n",
    "# is our preprocessed input.\n",
    "# As that is a relative complex task, that is regarded it's owy model.\n",
    "\n",
    "preprocessed_layers = layers.Concatenate()(encoded_features) #encoded_features\n",
    "preprocesessing_model = tf.keras.Model(all_inputs, preprocessed_layers)\n",
    "preprocesessing_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# The main model will contain 2 more sub-models next to the preprocessing one.\n",
    "# One of them represents different concepts, the other one how important a certain concept is,\n",
    "# depending on the input.\n",
    "\n",
    "# As both models create a separate output, to create a prediction, their outputs have to be combined.\n",
    "\n",
    "# A simple way to aggregate the output is to just multiply the concepts with their weights\n",
    "\n",
    "# While the name is *SUM*-aggregator, it is not actually summed RN #TODO rename it\n",
    "# the summing is done in the main model, as the (in the usual sense prmature) output of this function helps interpret the model\n",
    "\n",
    "class SumAggregator():\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"Basic Aggregator that joins the concepts and relevances by summing their products.\n",
    "        -> weights every concept with its relevance the output is the sum\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    @staticmethod\n",
    "    def forward( concepts, relevances):\n",
    "        \"\"\"Forward pass of Sum Aggregator.\n",
    "\n",
    "        Aggregates concepts and relevances and returns the predictions for each class.\n",
    "\n",
    "        Parameters # TODO change this to appropriate TF variants\n",
    "        ----------\n",
    "        concepts : torch.Tensor\n",
    "            Contains the output of the conceptizer with shape (BATCH, NUM_CONCEPTS, DIM_CONCEPT=1).\n",
    "        relevances : torch.Tensor\n",
    "            Contains the output of the parameterizer with shape (BATCH, NUM_CONCEPTS, NUM_CLASSES).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        class_predictions : torch.Tensor\n",
    "            Predictions for each class. Shape - (BATCH, NUM_CLASSES)\n",
    "            \n",
    "        \"\"\"\n",
    "        #permuted = tf.transpose(relevances, perm=[0, 2])  # so that the number of concepts is at the end\n",
    "        #batch_matrix_matrix_product = tf.matmul(permuted, concepts)  # multiply all relevance scores\n",
    "        #       with their corresponding concepts activation\n",
    "        #aggregated = tf.squeeze(batch_matrix_matrix_product)  # squeeze(-1)  # remove the number of concepts\n",
    "        aggregated = tf.math.multiply(concepts, relevances)\n",
    "        return tf.nn.log_softmax(aggregated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the models structure, there are repetetive patterns.\n",
    "\n",
    "For readability those layers are combined into custom layers and models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# A combination of layers, common in the parameterizer\n",
    "\n",
    "class ParameterizerLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, out_shape, dropout_rate):\n",
    "        super(ParameterizerLayer, self).__init__()\n",
    "        self.para_lin = layers.Dense(out_shape, activation='linear')\n",
    "        self.para_drop = layers.Dropout(dropout_rate)\n",
    "        self.para_relu = layers.Dense(out_shape, activation=tf.keras.layers.LeakyReLU(alpha=0.01))\n",
    "        \n",
    "    \n",
    "    def call(self, input_tensor,  training=False):\n",
    "        x = self.para_lin(input_tensor)\n",
    "        if training:\n",
    "            x = self.para_drop(x, training=training)\n",
    "        x = self.para_relu(x)        \n",
    "        return x\n",
    "    \n",
    "# should minimize robustness loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# The purpose of this sub-model to the network is to assign weights to the conceptizers output.\n",
    "# One could say that every value in its output is a measure of how important a certain concept is.\n",
    "# Here a concept is one output node of the conceptizer,\n",
    "# the Parameterizers output at position x relates to the conceptizers output at that same index (x) \n",
    "\n",
    "class Parameterizer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, hidden_sizes, out_shape, dropout_rate):\n",
    "        super(Parameterizer, self).__init__()\n",
    "        self.para0 = ParameterizerLayer(hidden_sizes[0], dropout_rate)\n",
    "        self.para1 = ParameterizerLayer(hidden_sizes[1], dropout_rate)\n",
    "        self.para2 = ParameterizerLayer(hidden_sizes[2], dropout_rate)\n",
    "        self.para3 = ParameterizerLayer(hidden_sizes[3], dropout_rate)\n",
    "        self.para_lin = layers.Dense(out_shape, activation='linear')\n",
    "        self.para_drop = layers.Dropout(dropout_rate)\n",
    "\n",
    "        \n",
    "    \n",
    "    def call(self, input_tensor,  training=False):\n",
    "        x = self.para0(input_tensor)\n",
    "        x = self.para1(x)\n",
    "        x = self.para2(x)\n",
    "        x = self.para3(x)\n",
    "        x = self.para_lin(x)\n",
    "        if training:\n",
    "            x = self.para_drop(x, training=training)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This class stores the network itself\n",
    "\n",
    "# Basically: the input is preprocessed into normalized scalars and fed into \n",
    "# a) The conceptizer to learn which features go together to form underlying structures\n",
    "#    Here all concepts are = inputs so there is no explicit conceptizer necessary\n",
    "# b) The parameterizer to learn which concept is important in what situation.\n",
    "# a and b are then combined by the aggregator\n",
    "# the aggregators output is summed up to produce the prediction\n",
    "\n",
    "#TODO replace the singe neuron at the end by a sum function.\n",
    "\n",
    "class SENN(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, hidden_sizes, preprocessing_model, dropout_rate):\n",
    "        \n",
    "        #general (superclass) model constructor\n",
    "        super(SENN, self).__init__()\n",
    "        \n",
    "        #preprocess the input\n",
    "        self.input_layers = preprocessing_model    \n",
    "\n",
    "        input_shape = 55 #TODO update - 55 is the number of parameters after encoding (preprocessing_layers)\n",
    "        \n",
    "        # The model that gives a weight per feature (parameterizer)\n",
    "        out_shape = 55 \n",
    "        self.parameterizer = Parameterizer(hidden_sizes, out_shape, dropout_rate)  # out: out_shape\n",
    "        self.parameterizer.build((None, input_shape))\n",
    "        \n",
    "        # The way to combine the activation and weights (aggregator)\n",
    "        self.aggregator = SumAggregator(input_shape)\n",
    "        \n",
    "        #The output should be a probability between 0 and 1\n",
    "        self.sigmoid = layers.Dense(1, tf.keras.activations.sigmoid)\n",
    "        \n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        print(\"input_tensor:\", input_tensor)\n",
    "        pre_processed = input_tensor #self.input_layers(input_tensor) \n",
    "        \n",
    "        concepts = pre_processed\n",
    "        \n",
    "        print(\"Pre_processed:\", pre_processed)\n",
    "        relevances = self.parameterizer(pre_processed, training)\n",
    "        \n",
    "        #aggregate the output\n",
    "        aggr_output = self.aggregator.forward(concepts, relevances)\n",
    "        \n",
    "        summed = tf.keras.backend.sum(aggr_output, keepdims=True)\n",
    "        \n",
    "        probability = summed #self.sigmoid(summed)\n",
    "        \n",
    "        normalized = probability \n",
    "        \n",
    "        return normalized, concepts, relevances\n",
    "    \n",
    "    # customize the train function, base code from:\n",
    "    # https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# The network as object has to be initialized before use, the hidden_sizes are important to determine hoe big the network is going to be.\n",
    "# hidden_sizes must be exactly 4 elements long, each number gives the size of a hidden layer in the parameterizer.\n",
    "hidden_sizes = [100, 100, 50, 100]  # TODO fix this to actual size\n",
    "model = SENN(hidden_sizes, preprocesessing_model, dropout_rate=0.1)     \n",
    "\n",
    "model.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# do pre-processing of data separately\n",
    "processed_train_ds = train_ds.map(\n",
    "  lambda x, y: (preprocesessing_model(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "input_tensor: tf.Tensor(\n",
      "[[ 0.12296062 -0.29732367  0.          0.          1.          0.\n",
      "   0.          0.          0.          0.          1.          0.\n",
      "   0.          1.          0.          0.          0.          0.\n",
      "   1.          0.          0.          0.          0.          0.\n",
      "   1.          0.          0.          0.          1.          0.\n",
      "   0.          0.          1.          0.          0.          0.\n",
      "   0.          0.          1.          0.          0.          0.\n",
      "   1.          0.          0.          0.          0.          1.\n",
      "   0.          0.          0.          0.          0.          1.\n",
      "   0.        ]], shape=(1, 55), dtype=float32)\n",
      "Pre_processed: tf.Tensor(\n",
      "[[ 0.12296062 -0.29732367  0.          0.          1.          0.\n",
      "   0.          0.          0.          0.          1.          0.\n",
      "   0.          1.          0.          0.          0.          0.\n",
      "   1.          0.          0.          0.          0.          0.\n",
      "   1.          0.          0.          0.          1.          0.\n",
      "   0.          0.          1.          0.          0.          0.\n",
      "   0.          0.          1.          0.          0.          0.\n",
      "   1.          0.          0.          0.          0.          1.\n",
      "   0.          0.          0.          0.          0.          1.\n",
      "   0.        ]], shape=(1, 55), dtype=float32)\n",
      "aggregates: [-220.438126]\n",
      "relevances: [[-0.0438233651 -0.0538201891 -0.036885336 ... 0.0121825654 0.134615108 -0.0463136]]\n",
      "x_batch_train: [[0.12296062 -0.297323674 0 ... 0 1 0]]\n",
      "y_batch_train: [0]\n",
      "classification_loss: [0]\n",
      "WARNING: the classification_loss is not reliable!\n",
      "J_yx: [[0.00498105818 0.00959485676 0.00365025108 ... 0.00283690728 -0.0291443914 -0.0248445794]]\n",
      "robustness_loss: [[0.0488044247 0.0634150431 0.0405355878 ... -0.00934565812 -0.1637595 0.0214690194]]\n",
      "number of trainable_weights: 18\n",
      "gradients                  : [1 55]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "var and delta do not have the same shape[55,100] [55] [Op:ResourceApplyGradientDescent]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-7ab1da01f4ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m# the value of the variables to minimize the loss.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0msorted_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;31m# Log every 200 batches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduced_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m       return distribute_ctx.get_replica_context().merge_call(\n\u001b[0m\u001b[0;32m    546\u001b[0m           \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distributed_apply\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapply_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m     merge_fn = autograph.tf_convert(\n\u001b[0;32m   2714\u001b[0m         merge_fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m-> 2715\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2720\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[0;32m   2721\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2722\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2723\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2724\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[1;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[0;32m    630\u001b[0m           with ops.name_scope(\"update\" if eagerly_outside_functions else\n\u001b[0;32m    631\u001b[0m                               \"update_\" + var.op.name, skip_on_eager=True):\n\u001b[1;32m--> 632\u001b[1;33m             update_ops.extend(distribution.extended.update(\n\u001b[0m\u001b[0;32m    633\u001b[0m                 var, apply_grad_to_update_var, args=(grad,), group=False))\n\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2298\u001b[0m         fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m   2299\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2300\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2302\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2953\u001b[0m     \u001b[1;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2954\u001b[0m     \u001b[1;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2955\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2957\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   2959\u001b[0m     \u001b[1;31m# once that value is used for something.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2960\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2961\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2962\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2963\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m    606\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;34m\"apply_state\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_apply_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[0mapply_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"apply_state\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mapply_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\gradient_descent.py\u001b[0m in \u001b[0;36m_resource_apply_dense\u001b[1;34m(self, grad, var, apply_state)\u001b[0m\n\u001b[0;32m    146\u001b[0m           use_nesterov=self.nesterov)\n\u001b[0;32m    147\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m       return training_ops.resource_apply_gradient_descent(\n\u001b[0m\u001b[0;32m    149\u001b[0m           var.handle, coefficients[\"lr_t\"], grad, use_locking=self._use_locking)\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\training\\gen_training_ops.py\u001b[0m in \u001b[0;36mresource_apply_gradient_descent\u001b[1;34m(var, alpha, delta, use_locking, name)\u001b[0m\n\u001b[0;32m   1945\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1946\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1947\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1948\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1949\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6842\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6843\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6844\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: var and delta do not have the same shape[55,100] [55] [Op:ResourceApplyGradientDescent]"
     ]
    }
   ],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(processed_train_ds):\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # Logits for this minibatch are stored in 'aggregates'\n",
    "            aggregates, concepts, relevances = model(x_batch_train, training=True)  \n",
    "            \n",
    "            #making sure the formats match\n",
    "            #TODO this only works for batch_size of 1\n",
    "            aggregates = aggregates[0]\n",
    "            \n",
    "            #make take record all actions that happen to the variables:\n",
    "            tape.watch(aggregates)\n",
    "            tape.watch(concepts)\n",
    "            tape.watch(relevances)\n",
    "            \n",
    "            ########## compute the losses ##########\n",
    "            \n",
    "            # 1 look at the important variables\n",
    "            tf.print(\"aggregates:\", aggregates)\n",
    "            tf.print(\"relevances:\", relevances)\n",
    "            tf.print(\"x_batch_train:\", x_batch_train)\n",
    "            tf.print(\"y_batch_train:\", y_batch_train)\n",
    "            \n",
    "            # 2 the general classification loss\n",
    "            # convert the int tensors to float\n",
    "            float_y_batch_train = tf.cast(y_batch_train, dtype=tf.float32)\n",
    "            float_aggregates = tf.cast(aggregates, dtype=tf.float32)\n",
    "            \n",
    "            classification_loss = tf.nn.log_poisson_loss(targets= float_y_batch_train, log_input=float_aggregates)\n",
    "            tf.print(\"classification_loss:\", classification_loss)\n",
    "            tf.print(\"WARNING: the classification_loss is not reliable!\")\n",
    "            \n",
    "            # 3 robustness loss for the parameterizer\n",
    "            J_yx = tape.gradient(aggregates, x_batch_train, \n",
    "                                 unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
    "            \n",
    "            # if J_yx is a bunch of zeros that means that the graph is not connected. \n",
    "            if not tf.math.count_nonzero(J_yx) > 0:\n",
    "                raise ValueError(\"THE GRAPH IS NOT CONNECTED!\")\n",
    "            else:\n",
    "                tf.print(\"J_yx:\", J_yx) # looking at this step\n",
    "            robustness_loss = J_yx - relevances\n",
    "            tf.print(\"robustness_loss:\", robustness_loss)\n",
    "            #normed = tf.norm(robustness_loss, ord='fro')\n",
    "            #  TODO: want 'normed' to be in frobenius form but that seems to be unsupported? \n",
    "\n",
    "            \n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        #grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        grads = robustness_loss\n",
    "\n",
    "        tf.print(\"number of trainable_weights:\", len(model.trainable_weights))\n",
    "        tf.print(\"gradients                  :\", tf.shape(grads))\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        sorted_grads = zip(grads, model.trainable_weights)\n",
    "        optimizer.apply_gradients(sorted_grads)\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(classification_loss))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\\\n",
    "How can I constrain the output to something from 0 to 1 without disconnecting the graph?\\\n",
    "Why are there only 18 trainable weights when the model has 55 preprocessed inputs\\\n",
    "and the parameterizer has more than 18 outputs?\\\n",
    "Why is the classification loss so weird sometimes target = predicted should give 0, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6mNMfG6yEq5"
   },
   "source": [
    "Let's visualize our connectivity graph:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:48.331722Z",
     "iopub.status.busy": "2020-09-17T01:33:48.330973Z",
     "iopub.status.idle": "2020-09-17T01:33:48.644924Z",
     "shell.execute_reply": "2020-09-17T01:33:48.645441Z"
    },
    "id": "Y7Bkx4c7yEq5",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the Keras TensorBoard callback, used for the animated, interactive tensorboard visualizatioon\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "#This should plot the exhaustive graph, but is a bit unreliable\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CED6OStLyEq7"
   },
   "source": [
    "model.fit(train_ds, epochs=1, validation_data=val_ds, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:52.231471Z",
     "iopub.status.busy": "2020-09-17T01:33:52.230837Z",
     "iopub.status.idle": "2020-09-17T01:33:52.288853Z",
     "shell.execute_reply": "2020-09-17T01:33:52.289388Z"
    },
    "id": "T8N2uAdU2Cni",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize model in an interactive way\n",
    "#sadly only works until the preprocessing layers are over\n",
    "# tensorboard sometimes thinks there still is an instance running when it is not\n",
    "# fix that by deleting the contents of this folder or your equivalent of it\n",
    "# C:\\Users\\deisl\\AppData\\Local\\Temp\\.tensorboard-info\n",
    "\n",
    "%reload_ext tensorboard\n",
    "# rankdir='LR' is used to make the graph horizontal.\n",
    "#tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmZMnTKaCZda"
   },
   "source": [
    "## Inference on new data\n",
    "\n",
    "As the model contains all important parts, it should be able to work on any file of the right format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xkOlK8Zweeh"
   },
   "source": [
    "The model should be saved such that it can just be reloaded later.\\\n",
    "I will follow the tutorial [here](https://www.tensorflow.org/tutorials/keras/save_and_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:52.296839Z",
     "iopub.status.busy": "2020-09-17T01:33:52.296192Z",
     "iopub.status.idle": "2020-09-17T01:33:56.352943Z",
     "shell.execute_reply": "2020-09-17T01:33:56.352275Z"
    },
    "id": "QH9Zy1sBvwOH",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.save('my_pet_classifier')\n",
    "reloaded_model = tf.keras.models.load_model('my_pet_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D973plJrdwQ9"
   },
   "source": [
    "To get a prediction for a new sample, you can simply call `model.predict()`. There are just two things you need to do:\n",
    "\n",
    "1.   Wrap scalars into a list so as to have a batch dimension (models only process batches of data, not single samples)\n",
    "2.   Call `convert_to_tensor` on each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:56.360094Z",
     "iopub.status.busy": "2020-09-17T01:33:56.359400Z",
     "iopub.status.idle": "2020-09-17T01:33:56.669531Z",
     "shell.execute_reply": "2020-09-17T01:33:56.669015Z"
    },
    "id": "rKq4pxtdDa7i",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO this does not work\n",
    "\n",
    "sample = {\n",
    "    'Type': 'Cat',\n",
    "    'Age': 3,\n",
    "    'Breed1': 'Tabby',\n",
    "    'Gender': 'Male',\n",
    "    'Color1': 'Black',\n",
    "    'Color2': 'White',\n",
    "    'MaturitySize': 'Small',\n",
    "    'FurLength': 'Short',\n",
    "    'Vaccinated': 'No',\n",
    "    'Sterilized': 'No',\n",
    "    'Health': 'Healthy',\n",
    "    'Fee': 100,\n",
    "    'PhotoAmt': 2,\n",
    "}\n",
    "\n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "predictions = reloaded_model.predict(input_dict)\n",
    "prob = tf.nn.sigmoid(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This particular pet had a %.1f percent probability \"\n",
    "    \"of getting adopted.\" % (100 * prob)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "Old code for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a complex conceptizer would include:\n",
    "class ConceptizerLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, out_shape):\n",
    "        super(ConceptizerLayer, self).__init__()\n",
    "        self.lin = layers.Dense(out_shape, activation='linear')\n",
    "        self.relu = layers.Dense(out_shape, activation=tf.keras.layers.LeakyReLU(alpha=0.01))\n",
    "        \n",
    "    def call(self, input_tensor,  training=False):\n",
    "        #x = self.inp(input_tensor)\n",
    "        x = self.lin(input_tensor)\n",
    "        x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conceptizer is a submodel of our network.\n",
    "# The idea is, that it learns which combination of inputs are relevant together \n",
    "\n",
    "#For categorical data an identity mapping should be more interpretable,hence this class is no longer used\n",
    "class Conceptizer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, start_dim):\n",
    "        super(Conceptizer, self).__init__()\n",
    "        self.con0 = ConceptizerLayer(start_dim)\n",
    "        self.con1 = ConceptizerLayer(start_dim/2)\n",
    "        self.con2 = ConceptizerLayer(start_dim/4)\n",
    "        self.lin = layers.Dense(start_dim/4, activation='linear')\n",
    "\n",
    "        \n",
    "    \n",
    "    def call(self, input_tensor,  training=False):\n",
    "        x = self.con0(input_tensor)\n",
    "        x = self.con1(x)\n",
    "        x = self.con2(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "# should minimize reconstruction loss, but as this class it is not used any more, that was never implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While more compley concepts might work better, it will be most explainable if every factor is its own concept\n",
    "class IdentityConceptizer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, start_dim):\n",
    "        super(IdentityConceptizer, self).__init__()\n",
    "        self.identity = layers.Layer(start_dim) # standard implementation of call for Layer is identity\n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        return self.identity(input_tensor)\n",
    "    \n",
    "    #This is the layer implementation of the identityConceptizer \n",
    "    # As I switched to sequential models - this function is out of date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most basic conceptizer (buggy, just for concept)\n",
    "def get_identity_conceptizer(input_features):\n",
    "    conceptizer = keras.Sequential(\n",
    "        [\n",
    "            layers.Layer(input_features)\n",
    "        ]\n",
    "    )\n",
    "    conceptizer.compile(optimizer='adam', loss=tf.keras.losses.mean_squared_error) \n",
    "    # should minimize minimize reconstruction loss, but as it is non-trainable identity that is perfect already\n",
    "    return conceptizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "Just a note about how the trainstep inside the model would look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "#One step in training\n",
    "\n",
    "@tf.function\n",
    "def train_step(data):\n",
    "    # Unpack the data. Its structure depends on your model and\n",
    "    # on what you pass to `fit()`.\n",
    "\n",
    "    if len(data) == 3:\n",
    "        x, y, sample_weight = data\n",
    "        print(\"Warning: sample weight is not currently supported and will be ignored!\")\n",
    "    else:\n",
    "        x, y = data\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)  # Forward pass\n",
    "\n",
    "        \"\"\"\n",
    "        concepts = x\n",
    "\n",
    "        relevances = model.parameterizer(x, training=True)\n",
    "\n",
    "        #aggregate the output\n",
    "        aggr_output = model.aggregator.forward(concepts, relevances)\n",
    "\n",
    "        summed = tf.keras.backend.sum(aggr_output, keepdims=True)\n",
    "\n",
    "        probability = model.sigmoid(summed)\n",
    "\n",
    "        normalized = tf.keras.backend.round(probability)\n",
    "        y_pred = normalized[0]\n",
    "        tf.print(\"step y:\", y)\n",
    "        tf.print(\"step y_pred:\", y_pred)\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = keras.losses.binary_crossentropy(y, y_pred, from_logits=True)# TODO should be true, but gives shape error\n",
    "\n",
    "    tf.print(\"y:\", y)\n",
    "    tf.print(\"y_pred:\", y_pred)\n",
    "    tf.print(\"loss:\", loss)\n",
    "    # Compute gradients\n",
    "    trainable_vars = model.trainable_variables \n",
    "    \n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "    # Update weights\n",
    "    model.optimizer.apply_gradients(\n",
    "        zip(gradients, trainable_vars)\n",
    "    )\n",
    "\n",
    "    # Update metrics (includes the metric that tracks the loss)\n",
    "    accuracy_metric.update_state(y, y_pred) # TODO support sample_weight=sample_weight\n",
    "\n",
    "    # Compute metrics\n",
    "    loss_tracker.update_state(y, y_pred) #loss)\n",
    "    accuracy_metric.update_state(y, y_pred)\n",
    "    bcl_metric.update_state(y, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss_tracker.result(), \n",
    "        \"accuracy\": accuracy_metric.result(), \n",
    "        \"bcl\": bcl_metric.result()\n",
    "    }\n",
    "\n",
    "# TODO overwrite test_step(self, data) to support model.evaluate()\n",
    "# gonna look something like this:\n",
    "\"\"\"\n",
    "def test_step(self, data):\n",
    "    # Unpack the data\n",
    "    x, y = data\n",
    "    # Compute predictions\n",
    "    y_pred = self(x, training=False)\n",
    "    # Updates the metrics tracking the loss\n",
    "    self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "    # Update the metrics.\n",
    "    self.compiled_metrics.update_state(y, y_pred)\n",
    "    # Return a dict mapping metric names to current value.\n",
    "    # Note that it will include the loss (tracked in self.metrics).\n",
    "    return {m.name: m.result() for m in self.metrics}\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing_layers.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

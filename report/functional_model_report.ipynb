{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg02FZzDyEqd"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:34.289130Z",
     "iopub.status.busy": "2020-09-17T01:33:34.288557Z",
     "iopub.status.idle": "2020-09-17T01:33:34.290255Z",
     "shell.execute_reply": "2020-09-17T01:33:34.290746Z"
    },
    "id": "2mapZ9afGJ69"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- fix model creation (preprocessing breaks tensor? https://tensorflow.google.cn/tutorials/load_data/csv)\n",
    "\n",
    "- name all layers\n",
    "\n",
    "- save model for loading afterwards (currently broken, might require named layers, might clash with concatenate layer)\n",
    "\n",
    "- make sure to mark unaltered cells and annotations as from the original tutorial\n",
    "\n",
    "- make sure the GPU is used\n",
    "\n",
    "- make model .fit and .evaluate work (if it does not)\n",
    "\n",
    "- make callbacks work during fitting (in custom loop)\n",
    "\n",
    "\n",
    "Maybe:\n",
    "\n",
    "- make a preprocessing model where the preprocessing is done when SENN is initialized\n",
    "(for portability)\n",
    "\n",
    "- augment data by just adding random noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMYQvJuBi7MS"
   },
   "source": [
    "# Classify structured data using Keras Preprocessing Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FaL4wnr22oy"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/preprocessing_layers.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/preprocessing_layers.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/structured_data/preprocessing_layers.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nna1tOKxyEqe"
   },
   "source": [
    "This tutorial demonstrates how to classify structured data (e.g. tabular data in a CSV). You will use [Keras](https://www.tensorflow.org/guide/keras) to define the model, and [preprocessing layers](https://keras.io/guides/preprocessing_layers/) as a bridge to map from columns in a CSV to features used to train the model. This tutorial contains complete code to:\n",
    "\n",
    "* Load a CSV file using [Pandas](https://pandas.pydata.org/).\n",
    "* Build an input pipeline to batch and shuffle the rows using [tf.data](https://www.tensorflow.org/guide/datasets).\n",
    "* Map from columns in the CSV to features used to train the model using Keras Preprocessing layers.\n",
    "* Build, train, and evaluate a model using Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5xkXCicjFQD"
   },
   "source": [
    "Note: This tutorial is similar to [Classify structured data with feature columns](https://www.tensorflow.org/tutorials/structured_data/feature_columns). This version uses new experimental Keras [Preprocessing Layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing) instead of `tf.feature_column`. Keras Preprocessing Layers are more intuitive, and can be easily included inside your model to simplify deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHxU1FMNpomc"
   },
   "source": [
    "## The Dataset\n",
    "\n",
    "For designing the network I use a smaller and simpler dataset. \n",
    "\n",
    "It is a simplified version of the PetFinder [dataset](https://www.kaggle.com/c/petfinder-adoption-prediction). There are several thousand rows in the CSV. Each row describes a pet, and each column describes an attribute. \n",
    "\n",
    "The goal is to predict if the pet will be adopted.\n",
    "\n",
    "Following is a description of this dataset. \\\n",
    "Notice there are both numeric and categorical columns. \n",
    "\n",
    "The free text column will be ignored.\n",
    "\n",
    "Column | Description| Feature Type | Data Type\n",
    "------------|--------------------|----------------------|-----------------\n",
    "Type | Type of animal (Dog, Cat) | Categorical | string\n",
    "Age |  Age of the pet | Numerical | integer\n",
    "Breed1 | Primary breed of the pet | Categorical | string\n",
    "Color1 | Color 1 of pet | Categorical | string\n",
    "Color2 | Color 2 of pet | Categorical | string\n",
    "MaturitySize | Size at maturity | Categorical | string\n",
    "FurLength | Fur length | Categorical | string\n",
    "Vaccinated | Pet has been vaccinated | Categorical | string\n",
    "Sterilized | Pet has been sterilized | Categorical | string\n",
    "Health | Health Condition | Categorical | string\n",
    "Fee | Adoption Fee | Numerical | integer\n",
    "Description | Profile write-up for this pet | Text | string\n",
    "PhotoAmt | Total uploaded photos for this pet | Numerical | integer\n",
    "AdoptionSpeed | Speed of adoption | Classification | integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjFbdBldyEqf"
   },
   "source": [
    "## Install and Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:34.295537Z",
     "iopub.status.busy": "2020-09-17T01:33:34.294862Z",
     "iopub.status.idle": "2020-09-17T01:33:35.675952Z",
     "shell.execute_reply": "2020-09-17T01:33:35.675219Z"
    },
    "id": "S_BdyQlPjfDW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from scikit-learn->sklearn) (1.5.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (1.18.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (1.1.4)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from pandas) (2020.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (1.33.2)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.23.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (50.3.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.11.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pydot in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from pydot) (2.4.7)\n",
      "Requirement already satisfied: pydotplus in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from pydotplus) (2.4.7)\n",
      "Requirement already satisfied: graphviz in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (0.14.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datetime in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (4.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from datetime) (2020.4)\n",
      "Requirement already satisfied: zope.interface in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from datetime) (5.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from zope.interface->datetime) (50.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (20.4)\n",
      "Requirement already satisfied: six in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from packaging) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from packaging) (2.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from keras) (1.5.4)\n",
      "Requirement already satisfied: six in c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages (from h5py->keras) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install tensorflow\n",
    "!pip install pydot\n",
    "!pip install pydotplus\n",
    "!pip install graphviz\n",
    "!pip install datetime\n",
    "!pip install packaging\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install for graph: https://graphviz.gitlab.io/download/\n",
    "maybe follow: https://bobswift.atlassian.net/wiki/spaces/GVIZ/pages/131924165/Graphviz+installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from datetime import datetime\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXvBvobayEqi"
   },
   "source": [
    "## Reading in the data\n",
    "\n",
    "The data is read into a pandas dataframe\n",
    "\n",
    "Again:\\\n",
    "As the real data is sensitive, large and expensive to use,\n",
    "for now I use a dummy dataset about adoption-speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:42.197790Z",
     "iopub.status.busy": "2020-09-17T01:33:42.197028Z",
     "iopub.status.idle": "2020-09-17T01:33:42.321623Z",
     "shell.execute_reply": "2020-09-17T01:33:42.320979Z"
    },
    "id": "qJ4Ajn-YyEqj",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\n",
    "csv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n",
    "\n",
    "tf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n",
    "                        extract=True, cache_dir='.')\n",
    "dataframe = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:42.343796Z",
     "iopub.status.busy": "2020-09-17T01:33:42.343010Z",
     "iopub.status.idle": "2020-09-17T01:33:42.352803Z",
     "shell.execute_reply": "2020-09-17T01:33:42.353374Z"
    },
    "id": "3uiq4hoIGyXI",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Fee</th>\n",
       "      <th>Description</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cat</td>\n",
       "      <td>3</td>\n",
       "      <td>Tabby</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>White</td>\n",
       "      <td>Small</td>\n",
       "      <td>Short</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>100</td>\n",
       "      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cat</td>\n",
       "      <td>1</td>\n",
       "      <td>Domestic Medium Hair</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Not Sure</td>\n",
       "      <td>Not Sure</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>I just found it alone yesterday near my apartm...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dog</td>\n",
       "      <td>1</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Male</td>\n",
       "      <td>Brown</td>\n",
       "      <td>White</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>Their pregnant mother was dumped by her irresp...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dog</td>\n",
       "      <td>4</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Short</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>150</td>\n",
       "      <td>Good guard dog, very alert, active, obedience ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dog</td>\n",
       "      <td>1</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>No Color</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Short</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>This handsome yet cute boy is up for adoption....</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  Age                Breed1  Gender Color1    Color2 MaturitySize  \\\n",
       "0  Cat    3                 Tabby    Male  Black     White        Small   \n",
       "1  Cat    1  Domestic Medium Hair    Male  Black     Brown       Medium   \n",
       "2  Dog    1           Mixed Breed    Male  Brown     White       Medium   \n",
       "3  Dog    4           Mixed Breed  Female  Black     Brown       Medium   \n",
       "4  Dog    1           Mixed Breed    Male  Black  No Color       Medium   \n",
       "\n",
       "  FurLength Vaccinated Sterilized   Health  Fee  \\\n",
       "0     Short         No         No  Healthy  100   \n",
       "1    Medium   Not Sure   Not Sure  Healthy    0   \n",
       "2    Medium        Yes         No  Healthy    0   \n",
       "3     Short        Yes         No  Healthy  150   \n",
       "4     Short         No         No  Healthy    0   \n",
       "\n",
       "                                         Description  PhotoAmt  AdoptionSpeed  \n",
       "0  Nibble is a 3+ month old ball of cuteness. He ...         1              2  \n",
       "1  I just found it alone yesterday near my apartm...         2              0  \n",
       "2  Their pregnant mother was dumped by her irresp...         7              3  \n",
       "3  Good guard dog, very alert, active, obedience ...         8              2  \n",
       "4  This handsome yet cute boy is up for adoption....         3              2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3zDbrozyEqq"
   },
   "source": [
    "## Creating the target variable\n",
    "\n",
    "I have to select the variable I want to train for and drop the columns that are not important or contain that information from the normal dataset.\n",
    "\n",
    "Valid for the example data:\n",
    "The task in the Kaggle competition was to predict the speed at which a pet will be adopted (e.g., in the first week, the first month, the first three months, and so on). Let's simplify this for our purposes. It is transformed into a binary classification problem:\n",
    "I simply predict whether the pet was adopted, or not.\n",
    "\n",
    "After modifying the label column, 0 will indicate the pet was not adopted, and 1 will indicate it was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:42.358975Z",
     "iopub.status.busy": "2020-09-17T01:33:42.358305Z",
     "iopub.status.idle": "2020-09-17T01:33:42.365151Z",
     "shell.execute_reply": "2020-09-17T01:33:42.364619Z"
    },
    "id": "wmMDc46-yEqq",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# In the original dataset \"4\" indicates the pet was not adopted.\n",
    "dataframe['target'] = np.where(dataframe['AdoptionSpeed']==4, 0, 1)\n",
    "\n",
    "# Drop un-used columns. (including our now target which can not be used for training)\n",
    "dataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataframe = dataframe.drop(columns=['Fee', 'PhotoAmt','Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\n",
    "#                    'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Breed1'])\n",
    "\n",
    "#for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Fee</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cat</td>\n",
       "      <td>3</td>\n",
       "      <td>Tabby</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>White</td>\n",
       "      <td>Small</td>\n",
       "      <td>Short</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cat</td>\n",
       "      <td>1</td>\n",
       "      <td>Domestic Medium Hair</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Not Sure</td>\n",
       "      <td>Not Sure</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dog</td>\n",
       "      <td>1</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Male</td>\n",
       "      <td>Brown</td>\n",
       "      <td>White</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dog</td>\n",
       "      <td>4</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Short</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dog</td>\n",
       "      <td>1</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>No Color</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Short</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  Age                Breed1  Gender Color1    Color2 MaturitySize  \\\n",
       "0  Cat    3                 Tabby    Male  Black     White        Small   \n",
       "1  Cat    1  Domestic Medium Hair    Male  Black     Brown       Medium   \n",
       "2  Dog    1           Mixed Breed    Male  Brown     White       Medium   \n",
       "3  Dog    4           Mixed Breed  Female  Black     Brown       Medium   \n",
       "4  Dog    1           Mixed Breed    Male  Black  No Color       Medium   \n",
       "\n",
       "  FurLength Vaccinated Sterilized   Health  Fee  PhotoAmt  target  \n",
       "0     Short         No         No  Healthy  100         1       1  \n",
       "1    Medium   Not Sure   Not Sure  Healthy    0         2       1  \n",
       "2    Medium        Yes         No  Healthy    0         7       1  \n",
       "3     Short        Yes         No  Healthy  150         8       1  \n",
       "4     Short         No         No  Healthy    0         3       1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sp0NCbswyEqs"
   },
   "source": [
    "## Spliting the dataframe into train, validation, and test\n",
    "\n",
    "The loaded dataset was a single file. It has to be split into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:42.370622Z",
     "iopub.status.busy": "2020-09-17T01:33:42.369987Z",
     "iopub.status.idle": "2020-09-17T01:33:42.377950Z",
     "shell.execute_reply": "2020-09-17T01:33:42.378325Z"
    },
    "id": "qT6HdyEwyEqt",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7383 train examples\n",
      "1846 validation examples\n",
      "2308 test examples\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_7uVu-xyEqv"
   },
   "source": [
    "## Input pipeline\n",
    "\n",
    "The dataframe is wrapped with [tf.data](https://www.tensorflow.org/guide/datasets).\n",
    "This is done to easily shuffle and batch the data. \n",
    "\n",
    "If the RAM is not sufficient, tf.data could be used directly to read it from disk in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:42.383853Z",
     "iopub.status.busy": "2020-09-17T01:33:42.383254Z",
     "iopub.status.idle": "2020-09-17T01:33:42.385369Z",
     "shell.execute_reply": "2020-09-17T01:33:42.384906Z"
    },
    "id": "7r4j-1lRyEqw",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=1):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('target')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYxIXH579uS9"
   },
   "source": [
    "The general pipeline for input is finished here.\n",
    "What does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:43.803710Z",
     "iopub.status.busy": "2020-09-17T01:33:42.388965Z",
     "iopub.status.idle": "2020-09-17T01:33:43.830128Z",
     "shell.execute_reply": "2020-09-17T01:33:43.830540Z"
    },
    "id": "tYiNH-QI96Jo",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:43.836836Z",
     "iopub.status.busy": "2020-09-17T01:33:43.836165Z",
     "iopub.status.idle": "2020-09-17T01:33:43.891242Z",
     "shell.execute_reply": "2020-09-17T01:33:43.890630Z"
    },
    "id": "nFYir6S8HgIJ",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['Type', 'Age', 'Breed1', 'Gender', 'Color1', 'Color2', 'MaturitySize', 'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Fee', 'PhotoAmt']\n",
      "A batch of ages: tf.Tensor([36 24  4 60  7], shape=(5,), dtype=int64)\n",
      "A batch of targets: tf.Tensor([0 0 1 0 0], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "[(train_features, label_batch)] = train_ds.take(1)\n",
    "print('Every feature:', list(train_features.keys()))\n",
    "print('A batch of ages:', train_features['Age'])\n",
    "print('A batch of targets:', label_batch )\n",
    "\n",
    "# TODO currently this is targeted towards the dummy -set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geqHWW54Hmte"
   },
   "source": [
    "The dataset returns a dictionary of column names (from the dataframe) that map to column values from rows in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v50jBIuj4gb"
   },
   "source": [
    "## Preprocessing layers\n",
    "\n",
    "I will have to adapt the pipelines when I replace the dummy-code, but afterwards I will be able to input plain string data etc from new data as well.\n",
    "\n",
    "Information about the pre-processing layers for easy access when I am there:\n",
    "\n",
    "*   [`Normalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization) - Feature-wise normalization of the data.\n",
    "*   [`CategoryEncoding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/CategoryEncoding) - Category encoding layer.\n",
    "*   [`StringLookup`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/StringLookup) - Maps strings from a vocabulary to integer indices.\n",
    "*   [`IntegerLookup`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/IntegerLookup) - Maps integers from a vocabulary to integer indices.\n",
    "\n",
    "A list of available preprocessing layers can be found [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twXBSxnT66o8"
   },
   "source": [
    "### Numeric columns\n",
    "A Normalization() layer ensures that each numeric feature has a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OosUh4kTsK_q"
   },
   "source": [
    "The `get_normalization_layer` function returns a keras layer.\n",
    "It applies featurewise normalization to numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:43.896825Z",
     "iopub.status.busy": "2020-09-17T01:33:43.896137Z",
     "iopub.status.idle": "2020-09-17T01:33:43.898143Z",
     "shell.execute_reply": "2020-09-17T01:33:43.898564Z"
    },
    "id": "D6OuEKMMyEq1",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for our feature.\n",
    "  normalizer = preprocessing.Normalization()\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:43.903065Z",
     "iopub.status.busy": "2020-09-17T01:33:43.902172Z",
     "iopub.status.idle": "2020-09-17T01:33:44.842415Z",
     "shell.execute_reply": "2020-09-17T01:33:44.842862Z"
    },
    "id": "MpKgUDyk69bM",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[ 0.4408663 ],\n",
       "       [ 0.4408663 ],\n",
       "       [-0.50762784],\n",
       "       [ 0.12470158],\n",
       "       [-0.50762784]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo_count_col = train_features['PhotoAmt']\n",
    "layer = get_normalization_layer('PhotoAmt', train_ds)\n",
    "layer(photo_count_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foWY00YBUx9N"
   },
   "source": [
    "TODO: If I will indeed have many numeric features (hundreds, or more), it would be more efficient to concatenate them first and use a single [normalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization) layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVD--2WZ7vmh"
   },
   "source": [
    "### Categorical columns\n",
    "\n",
    "In the dummy dataset, Type is represented as a string (e.g. 'Dog', or 'Cat'). Sadly, one can not feed strings directly to a model. The preprocessing layer takes care of representing strings as a one-hot vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWlkOPwMsxdv"
   },
   "source": [
    "The `get_category_encoding_layer` function returns a layer, mapping values from a vocabulary to integer indices and one-hot encodes the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:44.850381Z",
     "iopub.status.busy": "2020-09-17T01:33:44.847119Z",
     "iopub.status.idle": "2020-09-17T01:33:44.851920Z",
     "shell.execute_reply": "2020-09-17T01:33:44.852447Z"
    },
    "id": "GmgaeRjlDoUO",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  # Create a StringLookup layer which will turn strings into integer indices\n",
    "  if dtype == 'string':\n",
    "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "  else:\n",
    "    index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Create a Discretization for our integer indices.\n",
    "  encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = feature_ds.map(index)\n",
    "\n",
    "  # Learn the space of possible indices.\n",
    "  encoder.adapt(feature_ds)\n",
    "\n",
    "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "  # layer so we can use them, or include them in the functional model later.\n",
    "  return lambda feature: encoder(index(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:44.862532Z",
     "iopub.status.busy": "2020-09-17T01:33:44.861894Z",
     "iopub.status.idle": "2020-09-17T01:33:45.538315Z",
     "shell.execute_reply": "2020-09-17T01:33:45.537801Z"
    },
    "id": "X2t2ff9K8PcT",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_col = train_features['Type']\n",
    "layer = get_category_encoding_layer('Type', train_ds, 'string')\n",
    "layer(type_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6eDongw8knz"
   },
   "source": [
    "Often, you don't want to feed a number directly into the model, but instead use a one-hot encoding of those inputs. Consider raw data that represents a pet's age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:45.546513Z",
     "iopub.status.busy": "2020-09-17T01:33:45.545857Z",
     "iopub.status.idle": "2020-09-17T01:33:46.118888Z",
     "shell.execute_reply": "2020-09-17T01:33:46.118367Z"
    },
    "id": "7FjBioQ38oNE",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_col = train_features['Age']\n",
    "category_encoding_layer = get_category_encoding_layer('Age', train_ds,\n",
    "                                                      'int64', 5)\n",
    "category_encoding_layer(type_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiE0glOPkMyh"
   },
   "source": [
    "## Choosing and preparing columns to use\n",
    "\n",
    "While we can deal with all types of data, we have to make a list of all columns for each type.\\\n",
    "That way I am able to define which layer needs to be treated how\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:46.142976Z",
     "iopub.status.busy": "2020-09-17T01:33:46.123925Z",
     "iopub.status.idle": "2020-09-17T01:33:46.180021Z",
     "shell.execute_reply": "2020-09-17T01:33:46.180497Z"
    },
    "id": "Rcv2kQTTo23h",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:46.190642Z",
     "iopub.status.busy": "2020-09-17T01:33:46.186870Z",
     "iopub.status.idle": "2020-09-17T01:33:46.368301Z",
     "shell.execute_reply": "2020-09-17T01:33:46.368852Z"
    },
    "id": "Q3RBa51VkaAn",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "##bookkeeping for interpretation\n",
    "output_sizes = {}\n",
    "\n",
    "numerical_features = ['PhotoAmt', 'Fee']\n",
    "# Numeric features.\n",
    "for header in numerical_features:  # TODO use all headers in UMC set minus the ones I know are something else\n",
    "    numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "    normalization_layer = get_normalization_layer(header, train_ds)\n",
    "    encoded_numeric_col = normalization_layer(numeric_col)\n",
    "    all_inputs.append(numeric_col)\n",
    "    encoded_features.append(encoded_numeric_col)\n",
    "    output_sizes[header] = encoded_numeric_col.get_shape()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:46.379746Z",
     "iopub.status.busy": "2020-09-17T01:33:46.378953Z",
     "iopub.status.idle": "2020-09-17T01:33:46.551415Z",
     "shell.execute_reply": "2020-09-17T01:33:46.550766Z"
    },
    "id": "1FOMGfZflhoA",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Categorical features encoded as integers.\n",
    "\n",
    "# TODO at the UMC data, this will be more common, some tests have a categorical scale \n",
    "# However, most of them can just be interpreted as normal numerical feature, so I won't have to overdo it\n",
    "\n",
    "\n",
    "categorical_int_features = ['Age']\n",
    "# Numeric features.\n",
    "for header in categorical_int_features:  \n",
    "    num_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "    encoding_layer = get_category_encoding_layer(header, train_ds, dtype='int64',\n",
    "                                                 max_tokens=5)\n",
    "    encoded_col = encoding_layer(num_col)\n",
    "    all_inputs.append(num_col)\n",
    "    encoded_features.append(encoded_col)\n",
    "    output_sizes[header] = encoded_col.get_shape()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:46.561248Z",
     "iopub.status.busy": "2020-09-17T01:33:46.560569Z",
     "iopub.status.idle": "2020-09-17T01:33:48.254306Z",
     "shell.execute_reply": "2020-09-17T01:33:48.254765Z"
    },
    "id": "K8C8xyiXm-Ie",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Categorical features encoded as string.\n",
    "categorical_cols = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\n",
    "                    'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Breed1'] \n",
    "# TODO replace this by reading the headings from the dataframe and substracting the hardcoded headings (that I know are something else))\n",
    "\n",
    "for header in categorical_cols:\n",
    "    categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "    encoding_layer = get_category_encoding_layer(header, train_ds, dtype='string',\n",
    "                                               max_tokens=5) # TODO maybe, this line has to be duplicated and slightly changed to accomodate for different max_tokens\n",
    "    encoded_categorical_col = encoding_layer(categorical_col)\n",
    "    all_inputs.append(categorical_col)\n",
    "    encoded_features.append(encoded_categorical_col)\n",
    "    output_sizes[header] = encoded_categorical_col.get_shape()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Currently I do not think the UMC data needs to be balanced.\n",
    "# It will be evaluated on the same dataset (though a different part of it)\n",
    "# We do not have a large number of samples that are underrepresented, probably causing large inaccura\n",
    "\n",
    "#use:\n",
    "#    https://www.tensorflow.org/tutorials/structured_data/imbalanced_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHSnhz2fyEq3"
   },
   "source": [
    "## The model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Age (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Type (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Color1 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Color2 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Gender (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "MaturitySize (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FurLength (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Vaccinated (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Sterilized (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Health (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Breed1 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "PhotoAmt (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Fee (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "integer_lookup_1 (IntegerLookup (None, 1)            0           Age[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_1 (StringLookup)  (None, 1)            0           Type[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_2 (StringLookup)  (None, 1)            0           Color1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_3 (StringLookup)  (None, 1)            0           Color2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_4 (StringLookup)  (None, 1)            0           Gender[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_5 (StringLookup)  (None, 1)            0           MaturitySize[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_6 (StringLookup)  (None, 1)            0           FurLength[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_7 (StringLookup)  (None, 1)            0           Vaccinated[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_8 (StringLookup)  (None, 1)            0           Sterilized[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_9 (StringLookup)  (None, 1)            0           Health[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_10 (StringLookup) (None, 1)            0           Breed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "normalization_1 (Normalization) (None, 1)            3           PhotoAmt[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "normalization_2 (Normalization) (None, 1)            3           Fee[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_2 (CategoryEn (None, 5)            0           integer_lookup_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_3 (CategoryEn (None, 4)            0           string_lookup_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_4 (CategoryEn (None, 5)            0           string_lookup_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_5 (CategoryEn (None, 5)            0           string_lookup_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_6 (CategoryEn (None, 4)            0           string_lookup_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_7 (CategoryEn (None, 5)            0           string_lookup_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_8 (CategoryEn (None, 5)            0           string_lookup_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_9 (CategoryEn (None, 5)            0           string_lookup_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_10 (CategoryE (None, 5)            0           string_lookup_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_11 (CategoryE (None, 5)            0           string_lookup_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_12 (CategoryE (None, 5)            0           string_lookup_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 55)           0           normalization_1[0][0]            \n",
      "                                                                 normalization_2[0][0]            \n",
      "                                                                 category_encoding_2[0][0]        \n",
      "                                                                 category_encoding_3[0][0]        \n",
      "                                                                 category_encoding_4[0][0]        \n",
      "                                                                 category_encoding_5[0][0]        \n",
      "                                                                 category_encoding_6[0][0]        \n",
      "                                                                 category_encoding_7[0][0]        \n",
      "                                                                 category_encoding_8[0][0]        \n",
      "                                                                 category_encoding_9[0][0]        \n",
      "                                                                 category_encoding_10[0][0]       \n",
      "                                                                 category_encoding_11[0][0]       \n",
      "                                                                 category_encoding_12[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 6\n",
      "Trainable params: 0\n",
      "Non-trainable params: 6\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The first step towards a working model\n",
    "# is our preprocessed input.\n",
    "# As that is a relative complex task, that is regarded it's owy model.\n",
    "\n",
    "preprocessed_layers = layers.Concatenate()(encoded_features) #encoded_features\n",
    "preprocesessing_model = tf.keras.Model(all_inputs, preprocessed_layers)\n",
    "preprocesessing_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the models structure, there are repetetive patterns.\n",
    "\n",
    "For readability those layers are combined into custom layers and models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# A combination of layers, common in the parameterizer\n",
    "\n",
    "class ParameterizerLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, out_shape, dropout_rate):\n",
    "        super(ParameterizerLayer, self).__init__()\n",
    "        self.para_lin = layers.Dense(out_shape, activation='linear')\n",
    "        self.para_drop = layers.Dropout(dropout_rate)\n",
    "        self.para_relu = layers.Dense(out_shape, activation=tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "        \n",
    "    \n",
    "    def call(self, input_tensor,  training=False):\n",
    "        x = self.para_lin(input_tensor)\n",
    "        if training:\n",
    "            x = self.para_drop(x, training=training)\n",
    "        x = self.para_relu(x)        \n",
    "        return x\n",
    "    \n",
    "# should minimize robustness loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functional model def\n",
    "\n",
    "#TODO no more static sizes\n",
    "batch_size = 1\n",
    "preprocessed_inputs_shape = 55 \n",
    "dropout_rate=0.1\n",
    "hidden_sizes = [40, 20, 15, 5]  # TODO fix this to actual size\n",
    "out_shape = preprocessed_inputs_shape\n",
    "\n",
    "input_shape = [batch_size, preprocessed_inputs_shape]\n",
    "input_layer = layers.Input(batch_shape = input_shape)\n",
    "\n",
    "x = input_layer\n",
    "###Parameterizer###\n",
    "x = ParameterizerLayer(hidden_sizes[0], dropout_rate)(x)\n",
    "x = ParameterizerLayer(hidden_sizes[1], dropout_rate)(x)\n",
    "x = ParameterizerLayer(hidden_sizes[2], dropout_rate)(x)\n",
    "x = ParameterizerLayer(hidden_sizes[3], dropout_rate)(x)\n",
    "x = layers.Dense(out_shape, activation='linear')(x)\n",
    "relevances = layers.Dropout(rate=dropout_rate, name=\"relevances\")(x)\n",
    "\n",
    "###Conceptizer###\n",
    "concepts = layers.Lambda(lambda t: t, name=\"concepts\")(input_layer)\n",
    "\n",
    "###Aggregator###\n",
    "\n",
    "aggregated = layers.multiply([relevances, concepts])\n",
    "aggregated = layers.Lambda(lambda t: tf.keras.backend.sum(t, axis=-1))(aggregated)\n",
    "aggregated = layers.Lambda(lambda t: tf.keras.activations.sigmoid(t), name=\"output\")(aggregated)\n",
    "\n",
    "out_layer = [aggregated, concepts, relevances]\n",
    "\n",
    "functional_model = tf.keras.Model(inputs=input_layer, outputs=out_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom fun with more inputs\n",
    "\n",
    "def get_custom_loss(some_other_argument):\n",
    "    \n",
    "    def custom_loss(y_true, y_pred): \n",
    "        loss = 0\n",
    "        loss = loss + some_other_argument\n",
    "        loss = keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "        return loss\n",
    "    \n",
    "    return custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_loss(y_true, y_pred):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(1, 55)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "parameterizer_layer (Parameteri (1, 40)              3880        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "parameterizer_layer_1 (Paramete (1, 20)              1240        parameterizer_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "parameterizer_layer_2 (Paramete (1, 15)              555         parameterizer_layer_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "parameterizer_layer_3 (Paramete (1, 5)               110         parameterizer_layer_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (1, 55)              330         parameterizer_layer_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "relevances (Dropout)            (1, 55)              0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concepts (Lambda)               (1, 55)              0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (1, 55)              0           relevances[0][0]                 \n",
      "                                                                 concepts[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (1,)                 0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output (Lambda)                 (1,)                 0           lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 6,115\n",
      "Trainable params: 6,115\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loss_dict = {\n",
    "    \"relevances\": keras.losses.mean_absolute_error, #get_custom_loss(some_other_argument=1),\n",
    "    \"output\": keras.losses.binary_crossentropy, #tf.nn.log_poisson_loss,\n",
    "    #\"concepts\": zero_loss\n",
    "}\n",
    "\n",
    "functional_model.compile(\n",
    "    optimizer=\"adam\", \n",
    "    loss=loss_dict,\n",
    "    loss_weights=[1, 5, 0],\n",
    "    metrics= ['accuracy']\n",
    ")  \n",
    "functional_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# do pre-processing of data separately\n",
    "processed_train_ds = train_ds.map(\n",
    "  lambda x, y: (\n",
    "      tf.cast(preprocesessing_model(x), dtype=tf.float32), \n",
    "      tf.cast(y, dtype=tf.float32)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:48.331722Z",
     "iopub.status.busy": "2020-09-17T01:33:48.330973Z",
     "iopub.status.idle": "2020-09-17T01:33:48.644924Z",
     "shell.execute_reply": "2020-09-17T01:33:48.645441Z"
    },
    "id": "Y7Bkx4c7yEq5",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "# Define the Keras TensorBoard callback, used for the animated, interactive tensorboard visualizatioon\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "#This should plot the exhaustive graph, but is a bit unreliable\n",
    "tf.keras.utils.plot_model(functional_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ParameterizerLayer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/3\n",
      "   1/7383 [..............................] - ETA: 0s - loss: 0.6955 - output_loss: 0.6955 - relevances_loss: 0.0483 - output_accuracy: 0.0000e+00 - concepts_accuracy: 0.0000e+00 - relevances_accuracy: 0.0000e+00WARNING:tensorflow:From c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0320s). Check your callbacks.\n",
      "7383/7383 [==============================] - 7s 969us/step - loss: 0.5444 - output_loss: 0.5444 - relevances_loss: 0.7113 - output_accuracy: 0.7363 - concepts_accuracy: 0.0630 - relevances_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "7383/7383 [==============================] - 7s 993us/step - loss: 0.5230 - output_loss: 0.5230 - relevances_loss: 0.7040 - output_accuracy: 0.7452 - concepts_accuracy: 0.0630 - relevances_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "7383/7383 [==============================] - 7s 901us/step - loss: 0.5203 - output_loss: 0.5203 - relevances_loss: 0.6991 - output_accuracy: 0.7517 - concepts_accuracy: 0.0630 - relevances_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x235a7a5bb80>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.print(\"processed_train_ds shape:\", processed_train_ds.take(0))\n",
    "functional_model.fit(processed_train_ds, epochs=3, callbacks=tensorboard_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\\\n",
    "1: The error refers to this:\\\n",
    "I provide a gradient for every input feature (55),\\\n",
    "    that makes sense as my parameterizer has output-dimensions of one per input.\\\n",
    "However, it wants a second dimension of 10,\\\n",
    "    which is de dimension of the FIRST layer of the parameterizer, NOT the last.\\\n",
    "I do not understand that or what to do about it.\\\n",
    "\\\n",
    "2: How can I constrain the output to something from 0 to 1 without disconnecting the graph?\\\n",
    "3: Why are there only 18 trainable weights when the model has 55 preprocessed inputs\\\n",
    "   and the parameterizer has more than 18 outputs?\\\n",
    "4: How do I scale up to a larger batch_size, given that now I have to use the [0] trick to make it work?\\\n",
    "5: Why is the classification loss so weird sometimes target = predicted should give 0, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6mNMfG6yEq5"
   },
   "source": [
    "Let's visualize our connectivity graph:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do pre-processing of data separately\n",
    "processed_test_ds = test_ds.map(\n",
    "  lambda x, y: (\n",
    "      tf.cast(preprocesessing_model(x), dtype=tf.float32), \n",
    "      tf.cast(y, dtype=tf.float32)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:52.231471Z",
     "iopub.status.busy": "2020-09-17T01:33:52.230837Z",
     "iopub.status.idle": "2020-09-17T01:33:52.288853Z",
     "shell.execute_reply": "2020-09-17T01:33:52.289388Z"
    },
    "id": "T8N2uAdU2Cni",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2308/2308 [==============================] - 1s 618us/step - loss: 0.5128 - output_loss: 0.5128 - relevances_loss: 0.6823 - output_accuracy: 0.7400 - concepts_accuracy: 0.0585 - relevances_accuracy: 0.0000e+00\n",
      "Accuracy [0.5128363966941833, 0.5128363966941833, 0.6822918653488159, 0.7400346398353577, 0.05849220231175423, 0.0]\n"
     ]
    }
   ],
   "source": [
    "accuracy = functional_model.evaluate(processed_test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide x and y of test set\n",
    "x = []\n",
    "y_true = []\n",
    "\n",
    "for x_var, y_var in processed_test_ds:\n",
    "    x.append(x_var)\n",
    "    y_true.append(y_var[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic metrics\n",
    "def get_true_pos(y, pred, th):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == True) & (y == 1))\n",
    "\n",
    "\n",
    "def get_true_neg(y, pred, th):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == False) & (y == 0))\n",
    "\n",
    "\n",
    "def get_false_neg(y, pred, th):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == False) & (y == 1))\n",
    "\n",
    "\n",
    "def get_false_pos(y, pred, th):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == True) & (y == 0))\n",
    "\n",
    "def get_acc(tp, tn, fp, fn):\n",
    "    total = sum([tp, tn, fp, fn])\n",
    "    correct = sum([tp, tn])\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### based on coursera util.py for metrics\n",
    "\n",
    "\n",
    "def get_performance_metrics(y, pred, class_labels, threshold, \n",
    "                            tp=get_true_pos,\n",
    "                            tn=get_true_neg, fp=get_false_pos,\n",
    "                            fn=get_false_neg,\n",
    "                            acc=get_acc, prevalence=None, spec=None,\n",
    "                            sens=None, ppv=None, npv=None, auc=None, f1=None):\n",
    "\n",
    "    columns = [\"Label\", \"TP\", \"TN\", \"FP\", \"FN\", \"Accuracy\", \"Prevalence\",\n",
    "               \"Sensitivity\",\n",
    "               \"Specificity\", \"PPV\", \"NPV\", \"AUC\", \"F1\", \"Threshold\"]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    for i in range(len(class_labels)): ## i is the concerning class in each iteration\n",
    "        class_pred = pred[i]\n",
    "        #for separate classes:\n",
    "        #count_preds = len(class_pred) # the class was tried to predict as often as a prediction was made\n",
    "        #class_y = np.repeat(i, count_preds) # we filter for one class only anyway -> all the same\n",
    "        #for one class:\n",
    "        class_y = y[i]\n",
    "        ## get base metrics\n",
    "        true_p = round(tp(class_y, class_pred, threshold),3) if tp != None else \"Not Defined\"\n",
    "        true_n = round(tn(class_y, class_pred, threshold),3) if tn != None else \"Not Defined\"\n",
    "        false_p = round(fp(class_y, class_pred, threshold),3) if fp != None else \"Not Defined\"\n",
    "        false_n = round(fn(class_y, class_pred, threshold),3) if fn != None else \"Not Defined\"\n",
    "        \n",
    "        ## construct df for all data concerning class\n",
    "        row_data = {\n",
    "            \"Label\": class_labels[i],\n",
    "            \"TP\": true_p,\n",
    "            \"TN\": true_n,\n",
    "            \"FP\": false_p,\n",
    "            \"FN\": false_n,\n",
    "            \"Accuracy\": round(acc(true_p, true_n, false_p, false_n), 3) if acc != None else \"Not Defined\",\n",
    "            \"Prevalence\": round(prevalence(class_y), 3) if prevalence != None else \"Not Defined\",\n",
    "            \"Sensitivity\": round(sens(class_y, class_pred, threshold), 3) if sens != None else \"Not Defined\",\n",
    "            \"Specificity\": round(spec(class_y, class_pred, threshold), 3) if spec != None else \"Not Defined\",\n",
    "            \"PPV\": round(ppv(class_y, class_pred, threshold), 3) if ppv != None else \"Not Defined\",\n",
    "            \"NPV\": round(npv(class_y, class_pred, threshold), 3) if npv != None else \"Not Defined\",\n",
    "            \"AUC\": round(auc(class_y, class_pred), 3) if auc != None else \"Not Defined\",\n",
    "            \"F1\": round(f1(class_y, class_pred > threshold), 3) if f1 != None else \"Not Defined\",\n",
    "            \"Threshold\": round(threshold, 3)\n",
    "        }\n",
    "        tf.print(\"One row of metrics:\", row_data)\n",
    "        df = df.append(row_data, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def print_confidence_intervals(class_labels, statistics):\n",
    "    df = pd.DataFrame(columns=[\"Mean AUC (CI 5%-95%)\"])\n",
    "    for i in range(len(class_labels)):\n",
    "        mean = statistics.mean(axis=1)[i]\n",
    "        max_ = np.quantile(statistics, .95, axis=1)[i]\n",
    "        min_ = np.quantile(statistics, .05, axis=1)[i]\n",
    "        df.loc[class_labels[i]] = [\"%.2f (%.2f-%.2f)\" % (mean, min_, max_)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_curve(gt, pred, target_names, curve='roc'):\n",
    "    for i in range(len(target_names)):\n",
    "        if curve == 'roc':\n",
    "            curve_function = roc_curve\n",
    "            auc_roc = roc_auc_score(gt[:, i], pred[:, i])\n",
    "            label = target_names[i] + \" AUC: %.3f \" % auc_roc\n",
    "            xlabel = \"False positive rate\"\n",
    "            ylabel = \"True positive rate\"\n",
    "            a, b, _ = curve_function(gt[:, i], pred[:, i])\n",
    "            plt.figure(1, figsize=(7, 7))\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.plot(a, b, label=label)\n",
    "            plt.xlabel(xlabel)\n",
    "            plt.ylabel(ylabel)\n",
    "\n",
    "            plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n",
    "                       fancybox=True, ncol=1)\n",
    "        elif curve == 'prc':\n",
    "            precision, recall, _ = precision_recall_curve(gt[:, i], pred[:, i])\n",
    "            average_precision = average_precision_score(gt[:, i], pred[:, i])\n",
    "            label = target_names[i] + \" Avg.: %.3f \" % average_precision\n",
    "            plt.figure(1, figsize=(7, 7))\n",
    "            plt.step(recall, precision, where='post', label=label)\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n",
    "                       fancybox=True, ncol=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions\n",
    "y_pred = []\n",
    "stored_concepts = []\n",
    "stored_relevances = []\n",
    "\n",
    "for x_var in x:\n",
    "    aggregated, concepts, relevances = functional_model.predict(x_var)\n",
    "    y_pred.append(aggregated[0])\n",
    "    stored_concepts.append(concepts)\n",
    "    stored_relevances.append(relevances)\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "fpr_keras, tpr_keras, thresholds_rf = roc_curve(y_true, y_pred)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One row of metrics: {'AUC': 'Not Defined',\n",
      " 'Accuracy': 0.74,\n",
      " 'F1': 'Not Defined',\n",
      " 'FN': 66,\n",
      " 'FP': 534,\n",
      " 'Label': 'adopted',\n",
      " 'NPV': 'Not Defined',\n",
      " 'PPV': 'Not Defined',\n",
      " 'Prevalence': 'Not Defined',\n",
      " 'Sensitivity': 'Not Defined',\n",
      " 'Specificity': 'Not Defined',\n",
      " 'TN': 108,\n",
      " 'TP': 1600,\n",
      " 'Threshold': 0.5}\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Prevalence</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>PPV</th>\n",
       "      <th>NPV</th>\n",
       "      <th>AUC</th>\n",
       "      <th>F1</th>\n",
       "      <th>Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adopted</td>\n",
       "      <td>1600</td>\n",
       "      <td>108</td>\n",
       "      <td>534</td>\n",
       "      <td>66</td>\n",
       "      <td>0.74</td>\n",
       "      <td>Not Defined</td>\n",
       "      <td>Not Defined</td>\n",
       "      <td>Not Defined</td>\n",
       "      <td>Not Defined</td>\n",
       "      <td>Not Defined</td>\n",
       "      <td>Not Defined</td>\n",
       "      <td>Not Defined</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label    TP   TN   FP  FN  Accuracy   Prevalence  Sensitivity  \\\n",
       "0  adopted  1600  108  534  66      0.74  Not Defined  Not Defined   \n",
       "\n",
       "   Specificity          PPV          NPV          AUC           F1  Threshold  \n",
       "0  Not Defined  Not Defined  Not Defined  Not Defined  Not Defined        0.5  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_array = np.array(y_true)\n",
    "class_labels = ['adopted']\n",
    "metrics = get_performance_metrics([y_true_array], [y_pred], class_labels, 0.5)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(1, 55)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "parameterizer_layer (Parameteri (1, 40)              3880        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "parameterizer_layer_1 (Paramete (1, 20)              1240        parameterizer_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "parameterizer_layer_2 (Paramete (1, 15)              555         parameterizer_layer_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "parameterizer_layer_3 (Paramete (1, 5)               110         parameterizer_layer_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (1, 55)              330         parameterizer_layer_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "relevances (Dropout)            (1, 55)              0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concepts (Lambda)               (1, 55)              0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (1, 55)              0           relevances[0][0]                 \n",
      "                                                                 concepts[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (1,)                 0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output (Lambda)                 (1,)                 0           lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 6,115\n",
      "Trainable params: 6,115\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "functional_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 5276), started 1 day, 8:53:49 ago. (Use '!kill 5276' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7b27111567c779a1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7b27111567c779a1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize model in an interactive way\n",
    "#sadly only works until the preprocessing layers are over\n",
    "# tensorboard sometimes thinks there still is an instance running when it is not\n",
    "# fix that by deleting the contents of this folder or your equivalent of it\n",
    "# C:\\Users\\deisl\\AppData\\Local\\Temp\\.tensorboard-info\n",
    "\n",
    "%reload_ext tensorboard\n",
    "# rankdir='LR' is used to make the graph horizontal.\n",
    "#tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmZMnTKaCZda",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Inference on new data\n",
    "\n",
    "As the model contains all important parts, it should be able to work on any file of the right format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xkOlK8Zweeh"
   },
   "source": [
    "The model should be saved such that it can just be reloaded later.\\\n",
    "I will follow the tutorial [here](https://www.tensorflow.org/tutorials/keras/save_and_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:52.296839Z",
     "iopub.status.busy": "2020-09-17T01:33:52.296192Z",
     "iopub.status.idle": "2020-09-17T01:33:56.352943Z",
     "shell.execute_reply": "2020-09-17T01:33:56.352275Z"
    },
    "id": "QH9Zy1sBvwOH",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From c:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: preprocessing\\assets\n",
      "INFO:tensorflow:Assets written to: my_pet_classifier\\assets\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x00000235AFE3DB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x00000235AFE634C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x00000235AFE4A550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x00000235AFE56E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x00000235AFE63AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function recreate_function.<locals>.restored_function_body at 0x00000235AFE52310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000235AFE3DCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "preprocesessing_model.save('preprocessing')\n",
    "functional_model.save('my_pet_classifier')\n",
    "reloaded_preprocessing = tf.keras.models.load_model('preprocessing')\n",
    "reloaded_model = tf.keras.models.load_model('my_pet_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D973plJrdwQ9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To get a prediction for a new sample, you can simply call `model.predict()`. There are just two things you need to do:\n",
    "\n",
    "1.   Wrap scalars into a list so as to have a batch dimension (models only process batches of data, not single samples)\n",
    "2.   Call `convert_to_tensor` on each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#a simple, manual example\n",
    "\n",
    "sample = {\n",
    "    'Type': 'Cat',\n",
    "    'Age': 3,\n",
    "    'Breed1': 'Tabby',\n",
    "    'Gender': 'Male',\n",
    "    'Color1': 'Black',\n",
    "    'Color2': 'White',\n",
    "    'MaturitySize': 'Small',\n",
    "    'FurLength': 'Short',\n",
    "    'Vaccinated': 'No',\n",
    "    'Sterilized': 'No',\n",
    "    'Health': 'Healthy',\n",
    "    'Fee': 100,\n",
    "    'PhotoAmt': 2,\n",
    "}\n",
    "\n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "\n",
    "prediction_example = reloaded_preprocessing(input_dict)\n",
    "\n",
    "\n",
    "aggregated, concepts, relevances = functional_model.predict(prediction_example)\n",
    "prob = aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:33:56.360094Z",
     "iopub.status.busy": "2020-09-17T01:33:56.359400Z",
     "iopub.status.idle": "2020-09-17T01:33:56.669531Z",
     "shell.execute_reply": "2020-09-17T01:33:56.669015Z"
    },
    "id": "rKq4pxtdDa7i",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# create a dataframe of all stored outputs from testing\n",
    "stored_aggregates = y_pred \n",
    "outputs = pd.DataFrame(list(zip(stored_aggregates, stored_concepts, stored_relevances)),\n",
    "                      columns =['aggregated', 'concepts', 'relevances'])\n",
    "\n",
    "# creating a legend that contains which output belongs to which input\n",
    "out_legend = []\n",
    "\n",
    "for header in output_sizes:\n",
    "    size = output_sizes[header]\n",
    "    for layer in range(0, size):\n",
    "        out_legend.append(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which sample to look at \n",
    "\n",
    "def investigate_sample(output):\n",
    "    aggregated = output['aggregated']\n",
    "    concept = output['concepts']\n",
    "    relevance = output['relevances']\n",
    "\n",
    "    # if I multiply 0 inputs with the relevances first, only relevant parts will be shown\n",
    "    binary_concepts = [0 if concept==0 else 1 for concept in concepts[0]]\n",
    "    binary_concepts = np.array(binary_concepts)\n",
    "    polarized_relevances = np.multiply(binary_concepts, relevances)\n",
    "\n",
    "    filtered_output = [\n",
    "        (name,relevance) \n",
    "        for name, relevance in \n",
    "        zip(out_legend, polarized_relevances[0]) # TODO make this a dictionary\n",
    "        if not relevance==0\n",
    "    ]\n",
    "    \n",
    "    return aggregated, filtered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for abs(): 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-69-331e501d2fef>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;31m# such that all the aggregates are matched with their explanations\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m \u001B[0mabsolute_relevances\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mexplanations_frame\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;31m# TODO  make explanations_frame disregard negative symbols\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     17\u001B[0m \u001B[0maverage_relevance\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mabsolute_relevances\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;31m# TODO simple average of absolute_relevances\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\deisl\\desktop\\thesis\\adaptation\\senn\\cudaenv\\lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36mabs\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   9721\u001B[0m         \u001B[1;36m3\u001B[0m    \u001B[1;36m7\u001B[0m   \u001B[1;36m40\u001B[0m  \u001B[1;33m-\u001B[0m\u001B[1;36m50\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   9722\u001B[0m         \"\"\"\n\u001B[1;32m-> 9723\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   9724\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   9725\u001B[0m     def describe(\n",
      "\u001B[1;31mTypeError\u001B[0m: bad operand type for abs(): 'tuple'"
     ]
    }
   ],
   "source": [
    "# if \n",
    "# TODO filtered_output was a dictionary,\n",
    "# TODO and output was a param to investigate_sample()\n",
    "# one could loop through all outputs and make a dataframe with all the explanations\n",
    "\n",
    "aggregates = []\n",
    "explanations = []\n",
    "for output in outputs.iterrows():\n",
    "    aggregated, explanation = investigate_sample(output[1])\n",
    "    aggregates.append(aggregated)\n",
    "    explanations.append(explanation)\n",
    "explanations_frame = pd.DataFrame(explanations)#df from dictionary - something like this\n",
    "explanations_frame.add_column(aggregates) #TODO something like this, \n",
    "# such that all the aggregates are matched with their explanations\n",
    "\n",
    "absolute_relevances = explanations_frame.abs()# TODO  make explanations_frame disregard negative symbols\n",
    "average_relevance = absolute_relevances.mean(axis=0)# TODO simple average of absolute_relevances\n",
    "\n",
    "# TODO get average_relevances for entries specific to samples with high and low probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the absence of a feature is actually not really taken into account explicitly.\n",
    "# however: if a certain feature is absent, that shifts the relevances of other features.\n",
    "# thus one can say that in absence of a specific feature, the other, shown features become relevant\n",
    "\n",
    "# this does make the average values less informative - the complex relationships are ignored\n",
    "# a decision tree would be more suited to look at those relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# to inspect a specific sample:\n",
    "target_index = 89 # just enter the index of the sample\n",
    "output = outputs.loc[[target_index]]\n",
    "aggregated, explanation = investigate_sample(output)\n",
    "print(\n",
    "    \"This particular pet had a %.1f percent probability \"\n",
    "    \"of getting adopted.\" % (100 * aggregated)\n",
    ")\n",
    "tf.print(\"The explanations are: \\n\", explanation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This particular pet had a 66.3 percent probability of getting adopted.\n",
      "The explanations are: \n",
      " [('PhotoAmt', 0.23740412294864655),\n",
      " ('Fee', 0.03402990102767944),\n",
      " ('Age', -0.3418620228767395),\n",
      " ('Type', 0.22895996272563934),\n",
      " ('Color1', 0.24838268756866455),\n",
      " ('Color2', 0.03805258870124817),\n",
      " ('Gender', 0.11229459941387177),\n",
      " ('MaturitySize', 0.08923021703958511),\n",
      " ('FurLength', 0.003815448610112071),\n",
      " ('Vaccinated', 0.04055274650454521),\n",
      " ('Sterilized', -0.12769733369350433),\n",
      " ('Health', 0.12485720217227936),\n",
      " ('Breed1', -0.3344324231147766)]\r\n"
     ]
    }
   ],
   "source": [
    "# to inspect a specific sample:\n",
    "target_index = 89 # just enter the index of the sample\n",
    "output = outputs.loc[[target_index]]\n",
    "aggregated, explanation = investigate_sample(output)\n",
    "print(\n",
    "    \"This particular pet had a %.1f percent probability \"\n",
    "    \"of getting adopted.\" % (100 * aggregated)\n",
    ")\n",
    "tf.print(\"The explanations are: \\n\", explanation)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing_layers.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
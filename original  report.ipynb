{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Explaining Neural Networks: A review with extensions\n",
    "---\n",
    "\n",
    "<img src='senn/notebooks/img/SENN.png' style='width: 500px;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#setup)\n",
    "2. [Reproducing MNIST Results](#reproduce_mnist)\n",
    "3. [Reproducing COMPAS Results](#reproduce_compas)\n",
    "4. [Study of Robustness](#robustness_study)\n",
    "5. [Self-Explaining Neural Networks with Disentanglement](#DiSENN)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"setup\"></a>\n",
    "# 1. Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-42eb6c1550f8>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorchvision\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmake_grid\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from importlib import import_module\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from senn.models.losses import *\n",
    "from senn.models.parameterizers import *\n",
    "from senn.models.conceptizers import *\n",
    "from senn.models.aggregators import *\n",
    "from senn.models.senn import SENN, DiSENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from senn.datasets.dataloaders import get_dataloader\n",
    "from senn.utils.plot_utils import show_explainations, show_prototypes, plot_lambda_accuracy, get_comparison_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(filename):\n",
    "    config_path = Path('configs')\n",
    "    config_file = config_path / filename\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    return SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(config):\n",
    "    model_file = Path('results') / config.exp_name / \"checkpoints\" / \"best_model.pt\" \n",
    "    return torch.load(model_file, config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, dataloader, config):\n",
    "    accuracies = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, labels) in enumerate(dataloader):\n",
    "            x = x.float().to(config.device)\n",
    "            labels = labels.long().to(config.device)\n",
    "            y_pred, (concepts, relevances), _ = model(x)\n",
    "            accuracies.append((y_pred.argmax(axis=1) == labels).float().mean().item())\n",
    "    accuracy = np.array(accuracies).mean()\n",
    "    print(f\"Test Mean Accuracy: {accuracy * 100: .3f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"reproduce_mnist\"></a>\n",
    "# 2. Reproducing MNIST Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_config = get_config(\"mnist_lambda1e-4_seed29.json\")\n",
    "mnist_config.device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, mnist_test_dl = get_dataloader(mnist_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptizer = ConvConceptizer(**mnist_config.__dict__)\n",
    "parameterizer = ConvParameterizer(**mnist_config.__dict__)\n",
    "aggregator = SumAggregator(**mnist_config.__dict__)\n",
    "\n",
    "mnist_SENN = SENN(conceptizer, parameterizer, aggregator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_checkpoint = load_checkpoint(mnist_config)\n",
    "mnist_SENN.load_state_dict(mnist_checkpoint['model_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we run and evaluate the prediction performance of our model to check whether the results reported by the authors are reproducible or not. We are using the same hyperparameters and architecture that were reported by the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(mnist_SENN, mnist_test_dl, mnist_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained test accuracy closely matches the reported range of the authors which was 99.1% to 98.7% for different lambda values. We used $\\lambda = 1e-4$  for the tested model. For further hyperparameter details you can consult the config files used or the table in the report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function randomly samples from a test batch and produces explanations for their classification. With this experiment we want to qualitatively examine the interpretibility of the generated explanations. The model used in this experiment is the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_explainations(mnist_SENN, mnist_test_dl, 'mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevance scores of the explanations shown in the original paper are exclusively values close to either positive or negative one. The relevance scores in our explanations are more diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function finds the top nine prototypical test examples that activate a certain concept the most. This visualization method was also used by the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_prototypes(mnist_SENN, mnist_test_dl, 'activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper the prototypes of one concept are mostly of the same digit class. While this tendency can also be observed in some of our concept representations in other cases the concept is not prototypical for a certain digit class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we do not consider the vast majority of the explanations to be plausible and human interpretable. Therefore, this experiment is not reproducible. We believe that the authors picked only some of the few plausible examples by hand rather than showing randomly sampled results. We consider the poor interpretebility of the explanations a severe limitation of the framework. We argue that the lack of interpretability is at least partially caused by the way of representing the concepts. Selecting prototypes based only on the highest activation for each concept seems for us to be a crude method that fails to capture the real semantic meaning of the concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Concept Representation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors proposed two alternative methods for representing concepts as future work. As we argue in the previous experiment that the concept visualization has potential for improovement we tryed both of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with another prototype visualization method which we call '$\\textit{highest contrast}$'. The difference to the standard method (highest activation) is that the prototypes are not selected by only optimizing for highest activation for a certain concept but by also considering that all remaining concepts should be activated as little as possible at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function finds the prototypes with the highest contrast method using the same model as above for highest activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_prototypes(mnist_SENN, mnist_test_dl, 'contrast')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the prototypes are not more human interpretable than with the highest activation method. The efect of concepts not being represented by a single digit class is even stronger for the highest contrast method.\n",
    "Note that the two visualization methods are inconsistent in some cases because the interpretation of the same underlying concept changes. A good example of this is Concept 4. In the highest activation method it represented the digit class three but with the highest contrast method it is represented by many different digit classes that could be interpreted as sharing the property of roundness. \n",
    "Finding an interpretation of the concept representation is left to the user and is therefore highly subjective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function uses the second approach proposed by the authors. This time the concepts are not represented by prototypes but by the weights of the concolutional concept encoders. We use the last layer of the encoder because deeper layers represent more high-level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_prototypes(mnist_SENN, mnist_test_dl, 'filter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filters do not exhibit any human interpretable features. Considering the limited depth of the concept encoder, relatively small filter size and the simplicity of the data this is not surprising. However, we think a more suitable approach would be to visualize the filter activations of transposed convolutions applied on the concept specific filters respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tradeoff: Accuracy vs. Robustness "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we want to analyze the negative impact of the robustness loss rgularization on the accuracy. We therefore incrementally increase the amount of regularization and training the same model for each of the $\\lambda$-values. Because on MNIST the results were stable we only use one seed to report the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the config files of the five models that are used to generate the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = ['mnist_lambda1e-4_seed29.json', 'mnist_lambda1e-3_seed29.json', 'mnist_lambda1e-2_seed29.json', \n",
    "               'mnist_lambda1e-1_seed29.json', 'mnist_lambda1e-0_seed29.json']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The valid flag here is only used in this notebook to speed up calculations. The five models are not evaluated on the test set but instead the validation accuracies recorded during training are loaded. In the report we used the test accuracies which are almost the same as we did not do any hyperparameter search on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This will read the saved validation accuracies to speed up the process\n",
    "# in the report we evaluate on the test set\n",
    "_ = plot_lambda_accuracy(config_list, num_seeds=1, valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only a very small downward trend in accuracy up to the extreme value of $\\lambda = 1$ where the accuracy drops a lot. The general trend is consistent with the authors result but they do not report the huge drop for $\\lambda=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"reproduce_compas\"></a>\n",
    "# 3. Reproducing COMPAS Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compas_config = get_config(\"compas_lambda1e-4_seed555.json\")\n",
    "compas_config.device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, compas_test_dl = get_dataloader(compas_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptizer = IdentityConceptizer(**compas_config.__dict__)\n",
    "parameterizer = LinearParameterizer(**compas_config.__dict__)\n",
    "aggregator = SumAggregator(**compas_config.__dict__)\n",
    "\n",
    "compas_SENN = SENN(conceptizer, parameterizer, aggregator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compas_checkpoint = load_checkpoint(compas_config)\n",
    "\n",
    "compas_SENN.load_state_dict(compas_checkpoint['model_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as for MNIST we run and evaluate the prediction performance of our COMPAS model to check whether the results reported by the authors are reproducible or not. We are using the same hyperparameters and architecture that were reported by the authors for this dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(compas_SENN, compas_test_dl, compas_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained test accuracy closely matches the reported result of the authors (82%). We used $\\lambda = 1e-4$  for the tested model. For further hyperparameter details you can consult the config files used or the table in the report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness sake we also visualize two examples of explanations for the COMPAS dataset. As here no concepts are learned ( instead the raw inputs are used) the analysis of these explanations is not that interesting and therefore only shown in the reprt's appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_explainations(compas_SENN, compas_test_dl, 'compas', concept_names=compas_config.concept_names)\n",
    "show_explainations(compas_SENN, compas_test_dl, 'compas', concept_names=[f\"C{i}\" for i in range(11)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tradeoff: Accuracy vs. Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogously, to the regularization study performed on MNIST (see above) we do the same for the COMPAS dataset. The difference is that we train the model on three different seeds per lambda value. The used config files are listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [[\"compas_lambda1e-4_seed111.json\",\"compas_lambda1e-4_seed333.json\",\"compas_lambda1e-4_seed555.json\"],\n",
    "               [\"compas_lambda1e-3_seed111.json\",\"compas_lambda1e-3_seed333.json\",\"compas_lambda1e-3_seed555.json\"],\n",
    "               [\"compas_lambda1e-2_seed111.json\",\"compas_lambda1e-2_seed333.json\",\"compas_lambda1e-2_seed555.json\"],\n",
    "               [\"compas_lambda1e-1_seed111.json\",\"compas_lambda1e-1_seed333.json\",\"compas_lambda1e-1_seed555.json\"],\n",
    "               [\"compas_lambda1e-0_seed111.json\",\"compas_lambda1e-0_seed333.json\",\"compas_lambda1e-0_seed555.json\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we use the valid flag in this notebook to speed up computations (see explanation  in the respective MNIST experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This will read the saved validation accuracies to speed up the process\n",
    "# in the repoert we evaluate on the test set\n",
    "_ = plot_lambda_accuracy(config_list, num_seeds=3, valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the same downward trend in accuracy for increasing regularization that was reported by the authors. However, the magnitude of the decrease is a lot higher in our experiments. The authors only report a drop of about 4% points while in our experiment the difference in accuracy between the two extreme values is approximately 15% points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"robustness_study\"></a>\n",
    "# 4. Study of Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the robustness study the authors conducted by perturbating an input image with Guassian noise, we decided to take a different approach. Instead of using the input image with different levels of noise, we try to find semantically similar images in the dataset and plot their concepts and relevance scores. To cluster the images we train an autoencoder, whose architecture can be found in the appendix of the report, and perform k-nearest-neighbour search of the latent space (Euclidean distance) to get the images from the dataset that look most similar to the query image. As a result, we get k + 1 images for which we compare the concept activation and relevance scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained MNIST autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from senn.utils.MNIST_autoencoder import AETrainer, get_most_similar\n",
    "\n",
    "ae_trainer = AETrainer(mnist_test_dl, batch_size=200)\n",
    "ae_trainer.load_model(\"senn/utils/MNIST_autoencoder_pretrained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the latent vector for all the inputs from the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_latents = ae_trainer.get_latent_reps(mnist_test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test robustness for visually similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the closest vectors\n",
    "nearest_num = 5\n",
    "query_index = 0\n",
    "\n",
    "distances, indices = get_most_similar(latents=mnist_latents,\n",
    "                                      query=mnist_latents[query_index],\n",
    "                                      number=nearest_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input images from the testloader \n",
    "test_images = torch.stack([mnist_test_dl.dataset[index][0] for index in indices[0]])\n",
    "\n",
    "fig = get_comparison_plot(test_images, mnist_SENN)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment shows that the generated explanations are robust because the relevances as well as the concept activations are very similar for these perceptually similar test examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"DiSENN\"></a>\n",
    "# 5. Self-Explaining Neural Networks with Disentanglement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DiSENN incorporates a constrained variational inference framework on a \n",
    "SENN Concept Encoder to learn disentangled representations of the \n",
    "basis concepts as in [2]. The basis concepts are then independently\n",
    "sensitive to single generative factors leading to better interpretability \n",
    "and lesser overlap with other basis concepts. Such a strong constraint \n",
    "better fulfills the \"diversity\" desiderata for basis concepts\n",
    "in a Self-Explaining Neural Network.\n",
    "\n",
    "\n",
    "References  \n",
    "[1] Alvarez Melis, et al.\n",
    "\"Towards Robust Interpretability with Self-Explaining Neural Networks\" NIPS 2018  \n",
    "[2] Irina Higgins, et al. \n",
    "”β-VAE: Learning basic visual concepts with a constrained variational framework.” ICLR 2017. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disenn_config = get_config(\"MNIST_DiSENN.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, mnist_test_dl = get_dataloader(disenn_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptizer = VaeConceptizer(**disenn_config.__dict__)\n",
    "parameterizer = ConvParameterizer(**disenn_config.__dict__)\n",
    "aggregator = SumAggregator(**disenn_config.__dict__)\n",
    "\n",
    "disenn = DiSENN(conceptizer, parameterizer, aggregator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disenn_checkpoint = load_checkpoint(disenn_config)\n",
    "disenn.load_state_dict(disenn_checkpoint['model_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(disenn, mnist_test_dl, disenn_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Prototypes from Disentangled Concepts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(mnist_test_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find all images of digit 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(yb==3).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xb[18].cpu()\n",
    "fname = \"/digit3a.png\"\n",
    "disenn.explain(x, contrast_class=8, traversal_range=0.45,\n",
    "              gridsize=(1,6), col_span=3, figure_size=(18,3), show=True,\n",
    "              save_as=\"results/\"+disenn_config.exp_name+fname, use_cdf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xb[30].cpu()\n",
    "fname = \"/digit3b.png\"\n",
    "disenn.explain(x, contrast_class=8, traversal_range=0.45,\n",
    "              gridsize=(1,6), col_span=3, figure_size=(18,3), show=True,\n",
    "              save_as=\"results/\"+disenn_config.exp_name+fname, use_cdf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A random digit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xb[0].cpu()\n",
    "fname = \"/digit.png\"\n",
    "disenn.explain(x, contrast_class=0, traversal_range=0.45,\n",
    "              gridsize=(1,6), col_span=3, figure_size=(18,3), show=True,\n",
    "              save_as=\"results/\"+disenn_config.exp_name+fname, use_cdf=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}